# -*- coding: utf-8 -*-
"""FASHION CAMPUS COMPANY CHURN PREDICTION - BULGARIA TEAM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10DMXEivCMf1WdteyiXiIoTtYVeGxl_3W

# Churn Prediction & Customer Segmentation by Bulgaria Team

## Business Understanding

### Problem

Dengan jumlah user yang semakin meningkat, banyak user yang diacquired hanya untuk memenuhi jumlah user target dari perusahaan Fashion Campus. Tim Marketing diberikan tugas untuk membuat promo yang menarik untuk menarik perhatian user. Namun, hal ini berdampak dengan banyaknya user yang tidak organic, dimana mereka banyak yang tidak kembali lagi ke platform untuk melakukan transaksi. Churn rate dari user pun meningkat.

### Goal

* Mengetahui segmentasi customer
*  Mengetahui probabilitas churn dari customer untuk 1 bulan yang kedepan

### Objectives

*   Melakukan eksplorasi pada data untuk mengetahui insight dan karakteristik data
*   Melakukan segmentasi customer dengan metode RFM dan Algoritma K-Means Clustering, untuk mengetahui golongan customer sehingga tiap golongan dapat diberikan treatment yang tepat
*   Membuat model supervised machine learning untuk memprediksi probabilitas user untuk churn pada satu bulan kedepan

## Importing Library
"""

!pip install --upgrade matplotlib

import matplotlib
print('matplotlib: {}'.format(matplotlib.__version__))

! pip install kneed
! pip install squarify

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import matplotlib.ticker as mtick
# %matplotlib inline
import seaborn as sns
sns.set()
import ast
import json
from scipy import stats
from datetime import datetime, date
import datetime as dt
import random
import os
import re
import sys
import timeit
import string
import time
from time import time
from dateutil.parser import parse
import joblib
import math
import squarify
from IPython.display import display
from pandas.plotting import scatter_matrix
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)

from scipy import stats
from scipy.stats import norm
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score
from sklearn.metrics import classification_report, precision_recall_curve
from sklearn.metrics import auc, roc_auc_score, roc_curve
from sklearn.metrics import make_scorer, recall_score, log_loss
from sklearn.metrics import average_precision_score
from sklearn import feature_selection

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score
from sklearn.model_selection import cross_val_score

from sklearn import ensemble
from sklearn import svm, tree, linear_model, neighbors
from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis


from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import KFold
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.preprocessing import *
from sklearn.decomposition import PCA
from imblearn.datasets import make_imbalance
from statsmodels.tsa.seasonal import seasonal_decompose

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 60)

from google.colab import drive
drive.mount('/content/gdrive')

"""## Importing Dataset"""

path = '/content/gdrive/MyDrive/FINAL PROJECT BULGARIA/DATASET/'
cust = pd.read_csv(path+'customer.csv')
trans = pd.read_csv(path+'transactions.csv')
prod = pd.read_csv(path+'product.csv', error_bad_lines=False)
clk_str = pd.read_csv(path+'click_stream.csv')

"""## Data Understanding

**Customer Table** <br>
Tabel customer berisi data user/pelanggan yang terdaftar dalam aplikasi

*   customer_id = id unik tiap pelanggan
*   first_name = nama depan pelanggan
*   last_name = nama belakang pelanggan
*   username = username pelanggan terdaftar
*   email = email pelanggan terdaftar
*   gender = jenis kelamin (Male(M)/Female(F))
*   device_type = tipe OS device yang digunakan
*   device_id = id device pelanggan
*   device_version = Detail Versi Device Pelanggan
*   home_location_lat = Koordinat Latitude domisili pelanggan
*   home_location_long = Koordinat Longitude domisili pelanggan
*   home_location = Nama Provinsi domisili pelanggan
*   home_country = Nama Negara domisili pelanggan
*   first_join_date = Tanggal Registrasi Pelanggan

**Product Table** <br>
Table Produk memuat data tentang produk yang dijual, dalam hal ini adalah produk fashion

*   id = id produk fashion
*   gender = target produk berdasarkan gender
*   masterCategory = Master Kategori Fashion
*   subCategory = Sub Kategori Fashion
*   articleType = Tipe Fashion
*   baseColour = Warna Dasar Fashion
*   season = rekomendasi penggunaan fashion berdasarkan musim
*   year = Tahun Produksi
*   usage = tipe penggunaan fashion
*   productDisplayName = Nama Produk yang dipajang

**Transaction Table** <br>
Tabel transaksi memuat data tiap transaksi/pemesanan produk yang dilakukan oleh customer. Tiap customer bisa melakukan banyak pembelian pada banyak produk.
*   created_at = Waktu data dibuat
*   customer_id = id unik tiap pelanggan
*   booking_id = id pesanan (unique)
*   session_id = id sesi/kunjungan pelanggan di aplikasi (unique)
*   product_metadata = metadata produk yang dipesan
*   payment_method = metode pembayaran
*   payment_status = status pembayaran (Success / Failed)
*   promo_amount = besaran promo
*   promo_code = kode promo
*   shipment_fee = biaya pengiriman (ongkir)
*   shipment_date_limit = batas waktu pengiriman
*   shipment_location_lat = koordinat latitude lokasi pengiriman
*   shipment_location_long = koordinat longitude lokasi pengiriman
*   total_amount = total pembayaran

**Click Stream Table** <br>
Tabel Click Stream berisi data aktivitas penggunaan aplikasi yang dilakukan oleh user di tiap sesi atau pada saat mereka melakukan transaksi

*   session_id = id sesi/kunjungan
*   event_name = nama aktivitas
*   event_time = waktu terjadinya aktivitas
*   event_id = id aktivitas
*   traffic_source = sumber aktivitas (mobile/web)
*   event_metadata = metadata aktivitas pemesanan (kontennya serupa dengan product metadata)
"""

from IPython.display import display
# def resumetable(table):
tables = [cust, trans, prod, clk_str]
for i in tables:
  print('Jumlah Baris : ', i.shape[0])
  print('Jumlah Kolom : ', i.shape[1])
  print('============Info Table============')
  print(i.info())
  print('============Missing Value============')
  print(i.isna().sum())
  print('============Top 5 Rows============')
  display(i.head())

"""**Catatan:** <br>
*  Terdapat fitur `product_metadata` pda Transaction Table dan `event metadata` pada Click_Stream Table, yang berbentuk dictionary, ini perlu kita ekstrak isinya untuk membentuk fitur baru

*  Hampir semua kolom pada table product merupakan data kategorikal (kecuali kolom `year`), maka untuk mempermudah analisis dan eksplorasi kita perlu melakukan handling/imputasi jika terdapat missing value.
*   subCategory menginduk pada masterCategory
*   articleType merupakan spesifikasi dari subCategory <br>
masterCategory => subCategory => articleType

## Feature Engineering Phase 1
"""

# ===============================Customer Table===============================

# Mengubah tipe data pada fitur dari objek menjadi datetime
cust['first_join_date'] = pd.to_datetime(cust['first_join_date']).dt.tz_localize(None)
trans['created_at'] = pd.to_datetime(trans['created_at']).dt.tz_localize(None)
# Membuat fitur age untuk mengetahui usia customer
def age(born, today = dt.date(2022, 7, 31)):
    born = datetime.strptime(born, "%Y-%m-%d").date()
    return today.year - born.year - ((today.month,
                                      today.day) < (born.month,
                                                    born.day))
cust['age'] = cust['birthdate'].apply(age)
# Membuat fitur baru (Tahun Join, Bulan Join, dan Nama Hari Join) untuk memudahkan eksplorasi data
cust['join_year'] = cust.first_join_date.dt.year
cust['join_month'] = cust.first_join_date.dt.month
cust['join_dayname'] = cust.first_join_date.dt.day_name()
# Membuat fitur nama lengkap customer
cust['full_name'] = cust.first_name + ' ' + cust.last_name
# Mengubah tipe data customer_id menjadi object
cust['customer_id'] = cust['customer_id'].astype(str)
# Membuat fitur join_time untuk mengetahui berapa lama customer sudah join/terdaftar di aplikasi
cust['join_time']=trans.created_at.max() - cust.first_join_date

# =================================Product Table======================================

# Mengisi missing value pada BaseColur dengan 'Undefined'
prod.baseColour = prod.baseColour.fillna('Undefined')
# Mengisi missing value pada season dengan 'All Season'
prod.season = prod.season.fillna('All Season')
# Mengisi missing value pada kolom 'year' dengan modus kolom year
prod.year = prod.year.fillna(prod.year.mode()[0])
# Mengisi missing values pada usage dengan modus
prod.usage = prod.usage.fillna(prod.usage.mode()[0])
''' Jika melihat kolom productDisplayName,
maka kita dapat melihat bahwa isinya selalu mengandung kombinasi
informasi gender, baseColour dan articleType.
Maka untuk mengimputasi missing value pada productDisplayName,
kita dapat menggunakan kombinasi antar tiga kolom ini. '''
prod.productDisplayName = prod.productDisplayName.fillna(prod.gender + prod.baseColour + prod.articleType)
# Mengubah tipe data pada id menjadi object
prod['id'] = prod['id'].astype(str)
# Mengubah tipe data pada year menjadi integer
prod['year'] = prod['year'].astype(int)

# ===========================Transaction Table====================================

# Ekstrak Transaction Metadata
trans["product_metadata"]=trans["product_metadata"].apply(lambda x: ast.literal_eval(x))
trans=(
    trans.explode("product_metadata")
    .reset_index(drop=True)
)
trans=pd.concat([trans.drop(['product_metadata'],axis=1),pd.json_normalize(trans["product_metadata"])],axis=1)

# create feature year
trans['trans_year'] = trans.created_at.dt.year
# create feature month
trans['trans_month'] = trans.created_at.dt.month
# create feature date
trans['trans_date'] = trans.created_at.dt.day
# create feature day
trans['trans_day'] = trans.created_at.dt.dayofweek
# create feature dayname
trans['trans_dayname'] = trans.created_at.dt.day_name()
#create feature hour
trans['trans_hour'] = trans.created_at.dt.hour
#create feature daypart
hour_bin = [0,4,8,12,16,20,24]
l = ['Late Night', 'Early Morning','Morning','Noon','Evening','Night']
trans['trans_daypart'] = pd.cut(trans['trans_hour'], bins=hour_bin, labels=l, include_lowest=True)

# Mengubah tipe data pada `customer_id` menjadi object
trans['customer_id'] = trans['customer_id'].astype(str)
# Mengubah tipe data pada `product_id` menjadi object
trans['product_id'] = trans['product_id'].astype(str)
# Mengubah tipe data pada `item_price` menjadi float
trans['item_price'] = trans['item_price'].astype(float)
# Mengubah tipe data pada `promo_amount` menjadi float
trans['promo_amount'] = trans['promo_amount'].astype(float)
# Mengubah tipe data pada `shipment_fee` menjadi float
trans['shipment_fee'] = trans['shipment_fee'].astype(float)
# Mengubah tipe data pada `total_amount` menjadi float
trans['total_amount'] = trans['total_amount'].astype(float)

"""EDA Product Table"""

# Melihat jumlah produk berdasarkan Master Category
showMasterCategory = prod.groupby(['masterCategory'])['masterCategory'].agg(['count']).sort_values('count', ascending=False)
print(showMasterCategory)
# Melihat jumlah produk berdasarkan sub kategori
showSubCategory = prod.groupby(['subCategory'])['subCategory'].agg(['count']).sort_values('count', ascending=False)
print(showSubCategory)
# Melihat jumlah produk berdasarkan articleType
showArticleType = prod.groupby(['articleType'])['articleType'].agg(['count']).sort_values('count', ascending=False)
print(showArticleType)
# Melihat jumlah produk berdasarkan kategori dan article type
showProduct = prod.groupby(['masterCategory', 'subCategory', 'articleType'])['articleType'].agg(['count'])
# pd.set_option('display.max_rows', showProduct.shape[0]+1)
print(showProduct)
# Melihat jumlah produk berdasarkan kategori, musim, gender dan penggunaan
showProductSeasons = prod.groupby(['masterCategory', 'season', 'gender', 'usage'])['usage'].agg(['count'])
# pd.set_option('display.max_rows', showProductSeasons.shape[0]+1)
print(showProductSeasons)

"""#### Feature Engineering ClickStream"""

# Mengubah tipedata `event_time`
clk_str['event_time'] = pd.to_datetime(clk_str['event_time']).dt.tz_localize(None)
# Membuat beberapa fitur waktu
clk_str['dayname'] = clk_str.event_time.dt.day_name()
clk_str['hour'] = clk_str.event_time.dt.hour

hour_bin = [0,4,8,12,16,20,24]
l = ['Late Night', 'Early Morning','Morning','Noon','Evening','Night']
clk_str['daypart'] = pd.cut(clk_str['hour'], bins=hour_bin, labels=l, include_lowest=True)
clk_str.info()

clk_str.head()

"""#### Normalisasi click stream `event_metadata`"""

def literal_return(val):
    try:
        return ast.literal_eval(val)
    except (ValueError, SyntaxError) as e:
        return val

clk_str['event_metadata'] = clk_str['event_metadata'].apply(lambda x: literal_return(x))

clk_str1 = clk_str.drop('event_metadata',axis=1)

clk_str = pd.concat([clk_str1, pd.json_normalize(clk_str['event_metadata'])], axis=1)

clk_str

"""### Merge Tables

Merge tabel transaction dengan table customer dan table product untuk mendapatkan insight yang lebih lengkap.
Merge data dilakukan secara inner join. Pada tahap ini, customer yg tidak melakukan transaksi sama sekali, tidak akan masuk dalam eksplorasi lebih lanjut
"""

data = trans.merge(cust, on='customer_id')
data = data.merge(prod, left_on='product_id', right_on='id')
# data['promo_code'] = data.promo_code.fillna('')
# Renaming columns
data.rename(columns = {'first_name':'cust_first_name',
                       'last_name':'cust_last_name',
                       'gender_x':'cust_gender',
                       'gender_y':'prod_gender',
                       'home_location_lat':'cust_location_lat',
                       'home_location':'cust_location',
                       'home_location_long':'cust_location_long',
                       'home_country':'cust_country',
                       'first_join_date':'cust_join_date',
                       'age':'cust_age',
                       'full_name':'cust_full_name',
                       'year':'prod_year',
                       'usage':'prod_usage',
                       'baseColour':'prodBaseColour',
                       'articleType':'prodArticleType',
                       'masterCategory':'prodMasterCategory',
                       'subCategory':'prodSubCategory',
                       'season':'prodSeason',
                       'join_time':'cust_join_time'
                       }, inplace = True)
data.info()

# data.to_csv('/content/gdrive/MyDrive/FINAL PROJECT BULGARIA/DATASET/data_final.csv', index=False)

"""## EDA Phase 1
(Menguliti data, menjawab pertanyaan, mengambil insight baru, deteksi missing value, outliers, dll)

### Customer Table

#### Customer Gender

Melihat persebaran(jumlah) customer berdasarkan gender
"""

ax = cust.gender.replace({'M':'Pria','F': 'Wanita'}).value_counts(ascending=True, sort=True).plot(kind='barh',
                                            figsize=(10,8),
                                            color='royalblue'
                                            )
ax.bar_label(ax.containers[0])
ax.set_xlabel("Jumlah")
ax.set_title("Jumlah Customer berdasarkan Gender", fontsize=16)
plt.show()

"""Melihat persentase jumlah customer berdasarkan gender"""

plt.title("Customer Gender Percentage", fontsize=16)
cust.gender.value_counts().rename(index={'F':'Wanita', 'M':'Pria'}).plot(kind="pie",
                                                                         figsize=(8, 8),
                                                                         autopct="%.1f%%",
                                                                         fontsize=16,
                                                                        );
plt.show()

"""Berdasarkan diagram batang dan diagram lingkaran diatas, dapat diketahui bahwa customer wanita memiliki jumlah lebih banyak daripada customer pria.
Dari total populasi customer yakni 100,000, terdapat customer pria sebanyak 35,770  atau 35.8% dari total populasi, sedangkan customer wanita sebanyak 64,230 atau 64.2% dari total populasi.

#### Customer Home Location

Mencari tahu persebaran customer berdasarkan provinsi
"""

ax = cust.home_location.value_counts(ascending=True, sort=True).plot(kind='barh',
                                            figsize=(14,14),
                                            color='green'
                                            )
ax.bar_label(ax.containers[0])
ax.set_ylabel("Nama Provinsi")
ax.set_title("Distribusi Provinsi Customer", fontsize=16)
plt.show()

"""Dari diagram batang diatas, dapat diketahui bahwa Jakarta menjadi provinsi dengan jumlah customer terdaftar terbanyak, yakni mencapai 18,715. Sedangkan Riau menjadi provinsi yang memiliki jumlah customer terdaftar paling sedikit, yakni 132.

#### Customer Age Distribution

Mencari distribusi customer berdasarkan usia
"""

cust.age.describe()

"""Berdasarkan summary statistic dapat kita ketahui bahwa dari 100,000 customer, usia customer paling tua adalah 68 tahun, sedangkan yang paling muda adalah 6 tahun. Usia rata-rata customer adalah 26.3 tahun."""

plt.figure(figsize=(12, 8))
n, bins, patches = plt.hist(cust.age)
plt.xlabel("Age Interval")
plt.ylabel("Frequency")
plt.title("Customer Age Distribution", fontsize=16)
plt.axvline(x=cust.age.mean(), linewidth=3, color = 'r')
plt.show()

plt.figure(figsize=(12, 10))
sns.violinplot(x=cust['gender'].replace({'M':'Pria','F': 'Wanita'}), y=cust['age'])
plt.title('Distribusi Customer Berdasarkan Gender', fontsize=16)

plt.figure(figsize=(12, 8))
plt.hist(cust[cust.gender == 'F'].age, label='Wanita', alpha=.7, color='red', bins=15)
plt.hist(cust[cust.gender == 'M'].age, label="Pria", alpha=.4,
         edgecolor='black', color='yellow', bins=15)
plt.title("Distribusi Customer Berdasarkan Gender", fontsize=18)
plt.xlabel("Age Interval", fontsize=16)
plt.ylabel("Frequency", fontsize=16)
plt.legend(fontsize=18)
plt.show()

"""Dari grafik distribusi diatas, kita dapat melihat jumlah customer pada tiap rentang usia. Dapat kita ketahui bahwa frekuensi/jumlah customer paling banyak berada pada rentang usia 24-32 tahun, dengan rata-rata seperti yang disebutkan pada summary statistic sebelumnya yakni 26.3 tahun.

Selanjutnya kita akan melihat bagaimana customer mengelompok berdasarkan usia dan gender. Kita juga akan mendeteksi adanya outlier, atau nilai yang sangat berbeda pada data.
"""

sns.scatterplot(data=cust, x=cust.index, y=cust.age, hue="gender")

plt.figure(figsize=(12, 8))
sns.boxplot(data=cust, x='age')

"""Pada grafik scatter diatas, kita dapat melihat bagaimana data mengelompok berdasarkan usia dan gender. Terdapat customer yang berusia dibawah 10 tahun, ada juga yang berusia mendekati 70 tahun. Customer yang mendekati usia 70 tahun kita sebut sebagai outlier karena nilainya sangat berbeda, meskipun bisa jadi merupakan data asli/organik."""

cust.columns

"""#### Customer Device Type"""

plt.title("Dsitribusi Jenis Device", fontsize=16)
cust.device_type.value_counts().plot.pie(autopct="%.1f%%",
                                         fontsize=16,
                                         figsize=(8,8)
                                        );
plt.show()

"""Dari diagram lingkaran dapat diketahui bahwa pada saat join, customer terbagi menjadi dua macam tipe sistem operasi device (Android dan iOS), Android 76.6% sedangkan iOS 23.4%. Customer pengguna android terpantau lebih banyak dibanding pengguna iOS.

#### Customer Acquired

Kita akan mengetahui kapan aplikasi mendapatkan customer baru, atau kapan customer pertama kali melakukan registrasi

Membuat first_join_date menjadi index akan memudahkan dalam melakukan time series EDA.
"""

# menjadikan first_join_date sebagai index
cust = cust.set_index('first_join_date')
cust.tail(3)

cust.dtypes

cust.index

"""Melihat summary statistic dari tabel customer"""

cust.describe()

cust.head()

cust.index.value_counts().plot(linewidth=0.8,
                                   figsize=(16,6)
                                  )
ax.set_ylabel('Customer Baru')
plt.title('Distribusi Customer Baru berdasarkan Waktu', fontsize=16)
plt.show()

"""Dari grafik diatas, kita dapat melihat bahwa terjadi lonjakan customer baru diantara tahun 2021-2022. Mari kita lihat lebih jelas insight ini.

Mari kita fokuskan untuk membuat plotting grafik pada rentang waktu antara 1 Januari 2021 - 1 Januari 2022. Maka kita dapatkan grafik sebagai berikut:
"""

ax = cust.loc['2021-01-01':'2022-01-01'].index.value_counts().plot(figsize=(16,6))
ax.set_ylabel('Customer Baru')
plt.title('Distribusi Customer Baru : 1 Januari 2021 - 1 Januari 2022', fontsize=16)
plt.show()

"""Sekarang nampak lebih jelas, bahwa lonjakan customer baru terdapat pada bulan juli hingga september 2021. Mari kita buat lebih spesifik untuk mendapatkan insight yang lebih jelas."""

ax = cust.loc['2021-07-01':'2021-09-01'].index.value_counts().plot(figsize=(16,6))
ax.set_ylabel('Customer Baru')
plt.title('Distribusi Customer Baru : 7 Juli 2021 - 9 September 2021', fontsize=16)
plt.show()

ax = cust.loc['2021-07-25':'2021-08-05'].index.value_counts().plot(marker='o',
                                                       linewidth=3,
                                                       linestyle='-',
                                                       markersize=10,
                                                       markerfacecolor='r',
                                                       figsize=(16,6)
                                                      )
ax.set_ylabel('Customer Baru')
plt.title('Distribusi Customer Baru : 25 Juli 2021 - 5 Agustus 2021', fontsize=16)
plt.show()

"""Nampak bahwa terjadi lonjakan customer paling besar pada tanggal 31 Juli 2021."""

plt.title("Customer Acquired by Year", fontsize=16)
ax = cust.join_year.value_counts().sort_index(ascending=True).plot(x='join_year',
                                                                  color='royalblue',
                                                                  kind='bar',
                                                                  figsize=(16,6)
                                                                 )
ax.bar_label(ax.containers[0])
plt.show()

"""Dapat kita lihat trendline secara keseluruhan bahwa sejak 2016 hingga 2020, terjadi peningkatan jumlah customer yang diacquired secara konsisten, akan tetapi mulai tahun 2021 hingga tahun 2022 nampak terjadi penurunan jumlah customer yang diacquired. Customer paling banyak di acquired pada tahun 2021.

Melihat customer acquired berdasarkan bulan
"""

plt.title("Customer Acquired by Month", fontsize=16)
ax = cust.join_month.value_counts().sort_index(ascending=True).plot(x='join_month',
                                                                   color='green',
                                                                   kind='bar',
                                                                   figsize=(16,6)
                                                                  )
ax.bar_label(ax.containers[0])
plt.show()

"""Dapat diketahui bahwa customer acquired paling banyak tercatat pada bulan Juli.

Melihat customer acquired berdasarkan Day Name.
"""

plt.title("Customer Acquired by Day Name", fontsize=16)
ax = cust.join_dayname.value_counts().sort_values(ascending=True).plot(x='join_dayname',
                                                                     color='indigo',
                                                                     kind='bar',
                                                                     figsize=(16,6)
                                                                    )
ax.bar_label(ax.containers[0])
plt.show()

"""Nampak bahwa customer acquired paling banyak terjadi pada hari sabtu dan minggu (weekend)

### Product Table

#### Melihat persentase jenis pakaian berdasarkan gender
"""

prod['gender'].value_counts().plot.pie(autopct='%1.2f%%', figsize=(16, 8), fontsize=11).set_title("Product Gender Category", fontsize=20, weight='bold')

"""Dari grafik tersebut dapat dilihat product paling banyak dijual untuk kategori Men sebanyak 49.85% dan paling sedikit untuk kategori Girls sebanyak 1.47%

#### Melihat persentase produk berdasarkan Season
"""

ax = prod.season.value_counts(ascending=True, sort=True).plot(kind='barh',
                                            figsize=(10,8),
                                            color='magenta'
                                            )
ax.bar_label(ax.containers[0])
ax.set_xlabel("Jumlah")
ax.set_title("Jumlah Product berdasarkan Season", fontsize=16)
plt.show()

"""Produk yang dijual paling banyak diperuntukkan untuk Musim Panas (Summer) dan paling sedikit untuk Spring dilanjutkan All Season.

#### Persebaran BaseColour Product
"""

ax = prod.baseColour.value_counts(ascending=True, sort=True).plot(kind='barh',
                                            figsize=(10,12),
                                            color='purple'
                                            )
ax.bar_label(ax.containers[0])
ax.set_xlabel("Jumlah")
ax.set_title("Jumlah Product berdasarkan BaseColour", fontsize=16)
plt.show()

"""Nampak dari grafik, bahwa produk yang dijual paling banyak memiliki Warna Dasar Hitam (Black) dan paling sedikit warna Fluorescent Green.

### Transaction Table
"""

data.head()

"""Mengubah index transaction menjadi date (created_id) untuk memudahkan eksplorasi."""

data = data.set_index('created_at')
data.tail(3)

"""Menambah fitur baru (tahun transaksi, bulan transaksi, hari transaksi) untuk memperjelas eksplorasi"""

data.tail(3)

data.describe()

"""#### Transaction Distribution by Date"""

data.index.value_counts().plot(linewidth=0.8,
                                   figsize=(16,6)
                                  )
plt.show()

"""Nampak ada beberapa waktu dimana terjadi transaksi dalam jumlah besar.

#### Transaction Distribution by DayName
"""

plt.title("Transaction Distribution by Day Name", fontsize=16)
ax = data.trans_dayname.value_counts().sort_index().plot(x='trans_dayname',
                                                                     color='orange',
                                                                     kind='bar',
                                                                     figsize=(16,6)
                                                                    )
ax.bar_label(ax.containers[0])
plt.show()

crosstab_trans_dayname = pd.crosstab(index=data['trans_dayname'],
                        columns=data['cust_gender'])
crosstab_trans_dayname

plt.figure(figsize=(12,8))
ax = sns.countplot(data=data, x="trans_dayname", hue="cust_gender")
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.xlabel("Day Name")
plt.ylabel("Count")
plt.legend(bbox_to_anchor=(1.1, 0), loc='lower right', ncol=1)

plt.title("Transaction Distribution by Day Name & Customer Gender", fontsize=16)
plt.show()

"""Ternyata transaksi paling sering dilakukan di Hari Minggu, dan paling jarang dilakukan di hari Jumat

#### Transaction Distribution by DayPart
"""

plt.title("Transaction Distribution by Daypart", fontsize=16)
ax = data.trans_daypart.value_counts().sort_index().plot(x='trans_daypart',
                                                                     color='indigo',
                                                                     kind='bar',
                                                                     figsize=(16,6)
                                                                    )
ax.bar_label(ax.containers[0])
plt.show()

plt.figure(figsize=(12,8))
ax = sns.countplot(data=data, x="trans_daypart", hue="cust_gender")
ax.bar_label(ax.containers[0])
ax.bar_label(ax.containers[1])
plt.xlabel("Day Name")
plt.ylabel("Count")
plt.legend(bbox_to_anchor=(1.1, 0), loc='lower right', ncol=1)

plt.title("Transaction Distribution by Day Part & Customer Gender", fontsize=16)
plt.show()

"""Ternyata transaksi paling sering dilakukan pada dini hari, yakni pukul 00.00 - 04.00

#### Transaction Distribution by Hour
"""

plt.title("Transaction Distribution by Hour", fontsize=16)
ax = data.trans_hour.value_counts().sort_index(ascending=False).plot(x='trans_hour',
                                                                     color='royalblue',
                                                                     kind='barh',
                                                                     figsize=(16,10)
                                                                    )
ax.bar_label(ax.containers[0])
plt.show()

crosstab_trans_hour = pd.crosstab(index=data['trans_hour'],
                        columns=data['cust_gender'])
crosstab_trans_hour

ax = crosstab_trans_hour.sort_values(by='trans_hour', ascending=False).plot(kind='barh',
                    stacked=True,
                    colormap='tab10',
                    figsize=(14, 10))
ax.bar_label(ax.containers[1])
plt.legend(bbox_to_anchor=(1.1, 0), loc='lower right', ncol=1)
plt.xlabel("Transaction Count")
plt.ylabel("Hour")
plt.title("Transaction Distribution by Hour and Gender", fontsize=16)

plt.show()

"""#### Payment Method Distribution"""

plt.title("Payment Method Distribution", fontsize=16)
data.payment_method.value_counts().plot.pie(autopct="%.1f%%",
                                         fontsize=16,
                                         figsize=(8,8)
                                        );
plt.show()

plt.figure(figsize=(12,8))
plt.title("Payment Method Distribution", fontsize=16)
sns.countplot('payment_method',data=trans)

"""Metode pembayaran yang paling sering digunakan dalam transaksi adalah menggunakan Kartu Kredit, sedangkan yang paling jarang dilakukan adalah menggunakan e-wallet LinkAja."""

# Create subplots
fig, axes = plt.subplots(figsize=(12,12), nrows=3, ncols=2)
fig.subplots_adjust(hspace=3)
fig.suptitle('Payment Method Distribution by Age', fontsize=16)

# Generate histograms
for ax, (name, subdf) in zip(axes.flatten(), data.groupby('payment_method')):
    subdf.hist('cust_age', ax=ax)
    ax.set_title(name)
    ax.set_ylabel('jumlah')
    ax.set_xlabel('usia')

plt.show()

"""Nampak bahwa distribusi pelanggan yang melakukan transaksi berdasarkan metode pembayaran berada di rentang usia 20-30 tahun. Ini sesuai dengan distribusi usia customer yang kita lihat di awal.

#### Product Gender vs Customer Gender
"""

fig = plt.figure(figsize=(12, 8))
sns.countplot(data=data,
              x="prod_gender",
              hue="cust_gender")
plt.title("Product Gender vs Customer Gender", fontsize=16)
plt.xlabel('Product Gender')
plt.ylabel('Count')
plt.show()

"""Ternyata Jenis Produk yang paling laku adalah Kategori Men dengan pembeli paling banyak dari kalangan wanita. Produk berkategori girls diawal kita tahu merupakan produk yang paling sedikit dijual, akan tetapi kategori produk ini lebih laku daripada kategori Boys yang dijual lebih banyak.

#### Product Gender by Customer Age
"""

fig, axes = plt.subplots(figsize=(12,12), nrows=3, ncols=2)
fig.subplots_adjust(hspace=3)
fig.suptitle('Product Gender by Customer Age')

for ax, (name, subdf) in zip(axes.flatten(), data.groupby('prod_gender')):
    subdf.hist('cust_age', ax=ax)
    ax.set_title(name)
    ax.set_ylabel('jumlah')
    ax.set_xlabel('usia')

plt.show()

"""Lagi lagi pembeli paling banyak untuk tiap kategori produk berada di rentang usia 20-30 tahun.

#### Customer Gender vs Product Usage
"""

fig = plt.figure(figsize=(12, 8))
sns.countplot(data=data,
              x="prod_usage",
              hue="cust_gender")
plt.title("Product Usage vs Customer Gender", fontsize=16)
plt.xlabel('Product Gender')
plt.ylabel('Count')
plt.show()

"""Produk dengan tipe penggunaan casual adalah yang paling laku terjual, sedangkan produk dengan tipe home adalah yang paling jarang terjual

#### Product Purchased BaseColour Distribution
"""

fig = plt.figure(figsize=(12, 10))
sns.countplot(data=data,
              y="prodBaseColour", order = data.prodBaseColour.value_counts().index)
plt.title("Product Purchased Basecolour Distribution", fontsize=16)
plt.ylabel('Product BaseColour')
plt.xlabel('Count')
plt.show()

"""Produk dengan warna hitam adalah yang paling banyak dibeli, sesuai dengan fakta bahwa jumlah produk warna hitam adalah yang paling banyak dijual

#### Item Price vs Promo Amount
"""

plt.figure(figsize=(12,8))
sns.scatterplot(data=data, x="item_price", y="promo_amount", hue="cust_gender")

"""Nampak bahwa ternyata ada beberapa produk yang memiliki harga yang cenderung rendah, namun memiliki ongkos kirim yang lebih tinggi dibandingkan produk dengan harga yang jauh lebih tinggi. Ada banyak produk yang tidak memiliki ongkos kirim.

#### Payment Success Distribution
"""

plt.title("Payment Success Distribution", fontsize=16)
data.payment_status.value_counts().plot.pie(autopct="%.1f%%",
                                         fontsize=16,
                                         figsize=(8,8)
                                        );
plt.show()

"""Dari keseluruhan transaksi, nampak bahwa 4.3% nya mengalami kegagalan.

#### Payment Method vs Payment Status
"""

fig = plt.figure(figsize=(12, 8))
sns.countplot(data=data,
              x="payment_method",
              hue="payment_status")
plt.title("Payment Method vs Payment Status", fontsize=16)
plt.xlabel('Payment Status')
plt.ylabel('Count')
plt.show()

"""Failed Payment by Payment Method"""

plt.title("Failed Payment by Payment Method", fontsize=16)
data[data.payment_status == 'Failed'].payment_method.value_counts().plot.pie(autopct="%.1f%%",
                                         fontsize=16,
                                         figsize=(8,8)
                                        );
plt.show()

"""Jumlah Kegagalan dan kesuksesan transaksi sesuai dengan metode pembayaran, dimana paling tinggi berada pada pembayaran dengan kartu kredit.

#### Payment Succession Rate
"""

data_fail = data[data.payment_status == 'Failed'].payment_method.value_counts().rename_axis('payment_method').reset_index(name='counts')
data_succ = data[data.payment_status == 'Success'].payment_method.value_counts().rename_axis('payment_method').reset_index(name='counts')

data_rate = data_fail.merge(data_succ, on="payment_method")
data_rate = data_rate.rename(columns={'counts_x': 'failed', 'counts_y': 'success'})
data_rate.head()

data_rate['failed_rate'] = data_rate['failed']/(data_rate['failed']+data_rate['success'])*100
data_rate['success_rate'] = data_rate['success']/(data_rate['failed']+data_rate['success'])*100

data_rate

fig, ax = plt.subplots(figsize=(8, 6))

# add the plot
sns.barplot(y='payment_method', x='failed_rate', data=data_rate.sort_values(by='failed_rate', ascending=True), capsize=0.2, ax=ax)

# add the annotation
ax.bar_label(ax.containers[-1], fmt='%.3f%%', label_type='center', fontsize=14, color='white')

ax.set(title='Payment Method Failed Rate')
ax.set(ylabel='Payment Method')
ax.set(xlabel='Failed Rate')
plt.show()

"""Tingkat kegagalan transaksi paling tinggi adalah pada transaksi menggunakan e-wallet LinkAja. Terdapat korelasi, bahwa LinkAja merupakan metode pembayaran dengan minat paling rendah."""

fig, ax = plt.subplots(figsize=(8, 6))

# add the plot
sns.barplot(y='payment_method', x='success_rate', data=data_rate.sort_values(by='success_rate', ascending=False), capsize=0.2, ax=ax)

# add the annotation
ax.bar_label(ax.containers[-1], fmt='%.3f%%', label_type='center', fontsize=14, color='white')

ax.set(title='Payment Method Success Rate')
ax.set(ylabel='Payment Method')
ax.set(xlabel='Success Rate')
plt.show()

"""Tingkat keberhasilan transaksi paling tinggi berdasarkan metode pembayaran, adalah menggunakan e-wallet OVO.

### Click Stream Table

#### Event Distribution

Melihat aktivitas seluruh customer
"""

ax = clk_str['event_name'].value_counts().plot(kind='barh',
                                               figsize=(14,8),
                                               title="Distribusi Aktivitas Customer saat Menggunakan Aplikasi")
ax.set_xlabel("Event Count")
ax.set_ylabel("Event Name")

cross_tab_prop = pd.crosstab(index=clk_str['daypart'],
                             columns=clk_str['event_name'],
                             normalize="index")
cross_tab_prop

cross_tab = pd.crosstab(index=clk_str['daypart'],
                        columns=clk_str['event_name'])
cross_tab

cross_tab_prop.plot(kind='barh',
                    stacked=True,
                    colormap='tab10',
                    figsize=(12, 10))

plt.legend(bbox_to_anchor=(1.2, 0), loc='lower right', ncol=1)
plt.xlabel("Release Year")
plt.ylabel("Proportion")
plt.title("Event Distribution by Day Part", fontsize=16)

plt.show()

"""Dapat kita ketahui, bahwa aktivitas pelanggan paling banyak dihabiskan untuk aktivitas CLICK, sedangkan paling sedikit adalah ADD_PROMO. Aktivitas ADD_PROMO menjadi paling sedikit, mungkin bisa disebabkan karena memang ketersediaan promo yang sedikit, atau pelanggan yang memang tidak ingin/tidak sempat memakai kode promo yang ada, dibuktikan dengan cukup sedikit pelanggan yang melihat laman promo untuk melihat promo yang tersedia. Aktivitas pelanggan memasukkan item ke keranjang terpantau cukup tinggi (terbanyak ketiga), akan tetapi aktivitas booking masih rendah (terendah ketiga).

Melihat aktivitas customer yang melakukan transaksi
"""

yes_trans = clk_str[clk_str.session_id.isin(trans.session_id)]

ax = yes_trans['event_name'].value_counts().plot(kind='barh',
                                               figsize=(14,8),
                                               title="Distribusi Aktivitas Customer yang Melakukan Transaksi")
ax.set_xlabel("Event Count")
ax.set_ylabel("Event Name")

"""Melihat aktivitas customer yang **tidak** melakukan transaksi"""

no_trans = clk_str[~clk_str.session_id.isin(trans.session_id)]

ax = no_trans['event_name'].value_counts().plot(kind='barh',
                                               figsize=(14,8),
                                               title="Distribusi Aktivitas Customer yang tidak Melakukan Transaksi")
ax.set_xlabel("Event Count")
ax.set_ylabel("Event Name")

"""Dari distribusi diatas, dapat kita ketahui bahwa ternyata dari customer yang tidak melakukan transaksi, sebagian besar dari mereka masih melakukan aktivitas seperti scrolling, searching, melihat detail product, dan add to cart.

### Numerical Data Distribution

Mendeteksi lebih jelas tentang persebaran data di kolom tertentu, terutama untuk mendeteksi adanya outlier yang akan mempengaruhi performa model.
"""

data.dtypes

numeric_data = data.select_dtypes(include=[np.number])

numeric_data.columns

bc_data = data.copy()

"""#### Outlier Detection"""

for i in ["promo_amount", "shipment_fee", "quantity", "item_price", "cust_age"]:
    plt.figure(figsize=(12,8))
    plt.tight_layout()
    plt.gca().set(xlabel= i, ylabel='Frequency')
    sns.boxplot(x = data[i])

"""##### Price In Payment Method"""

plt.figure(figsize=(12,8))
sns.boxplot(x="payment_method",y="item_price",data=data)
plt.show()

"""##### Price In Device Type"""

plt.figure(figsize=(12,8))
sns.boxplot(x='device_type', y='item_price', data = data)
plt.show()

"""##### Price In Product Gender"""

plt.figure(figsize=(12,8))
sns.boxplot(x='prod_gender', y='item_price', data = data)
plt.show()

"""##### Price In Customer Gender"""

plt.figure(figsize=(12,8))
sns.boxplot(x='cust_gender', y='item_price', data = data)
plt.show()

"""## Feature Engineering Phase 2
(Attribute Selection, Imputation, Feature Scaling, Encoding, Feature Creation)
"""

def feature_engineering1(data):
  data = data.reset_index(drop=False)
  # data.drop(columns='index', inplace=True)
  # variables declaration
  shipment_fee_q3 = data.shipment_fee.quantile(.3)
  shipment_fee_q6 = data.shipment_fee.quantile(.6)
  shipment_fee_q9 = data.shipment_fee.quantile(.9)
  quantity_q8 = data.quantity.quantile(.8)
  quantity_q99 = data.quantity.quantile(.99)
  item_price_q3 = data.item_price.quantile(.3)
  item_price_q6 = data.item_price.quantile(.6)
  item_price_q9 = data.item_price.quantile(.9)
  jointime_q1 = data.cust_join_time.quantile(.1)
  jointime_q5 = data.cust_join_time.quantile(.5)

  # lists
  west_indo = ['Aceh', 'Jakarta Raya', 'Jawa Barat', 'Jawa Tengah', 'Jawa Timur',
             'Yogyakarta', 'Kalimantan Barat', 'Lampung', 'Kalimantan Tengah',
             'Kepulauan Riau', 'Sumatera Barat', 'Sumatera Utara', 'Bengkulu',
             'Sumatera Selatan', 'Banten', 'Jambi', 'Bangka Belitung', 'Riau']
  middle_indo = ['Kalimantan Selatan', 'Kalimantan Timur', 'Bali',
                'Nusa Tenggara Barat', 'Sulawesi Utara', 'Sulawesi Barat',
                'Sulawesi Selatan', 'Nusa Tenggara Timur', 'Gorontalo',
                'Sulawesi Tengah', 'Sulawesi Tenggara']
  east_indo = ['Maluku', 'Papua', 'Papua Barat', 'Maluku Utara']

  jawa_bali = ['Jakarta Raya', 'Jawa Barat', 'Jawa Tengah', 'Yogyakarta',
              'Jawa Timur', 'Banten', 'Bali']

  # customer age grouping
  def get_num_people_by_age_category(df):
      df["age_group"] = pd.cut(x=df['cust_age'], bins=[0,30,60,100], labels=["young","middle_aged","old"])
      return df
  #promo amount grouping
  def promo_amount(promo):
      if promo == 0:
          return "nothing"
      elif promo > 0 and promo <= 10000:
        return "medium"
      elif promo > 10000:
        return "high"
  # shipment fee grouping
  def shipment_fee(group):
    if group == 0:
      return 'nothing'
    elif group > 0 and group <= shipment_fee_q3:
      return 'cheap'
    elif group > shipment_fee_q3 and group <= shipment_fee_q6:
      return 'medium'
    elif group > shipment_fee_q6 and group <= shipment_fee_q9:
      return 'high'
    elif group > shipment_fee_q9:
      return 'very high'
  # quantity grouping
  def quantity_type(quantity):
    if quantity == quantity_q8:
      return 'single'
    elif quantity > quantity_q8 and quantity <= quantity_q99:
      return 'small'
    elif quantity > quantity_q99:
      return 'huge'
  #item price grouping
  def itemprice_type(price):
    if price <= item_price_q3:
      return 'cheap'
    elif price > item_price_q3 and price <= item_price_q6:
      return 'medium'
    elif price > item_price_q6 and price <= item_price_q9:
      return 'expensive'
    elif price > item_price_q9:
      return 'super_expensive'
  # customer join time grouping
  def cust_jointime(jointime):
    if jointime <= jointime_q1:
      return 'baby'
    elif jointime > jointime_q1 and jointime <= jointime_q5:
      return 'adult'
    elif jointime > jointime_q5:
      return 'old'
  # daypart grouping
  def daypart(hour):
      if hour in [2,3,4,5]:
          return "dawn"
      elif hour in [6,7,8,9]:
          return "morning"
      elif hour in [10,11,12,13]:
          return "noon"
      elif hour in [14,15,16,17]:
          return "afternoon"
      elif hour in [18,19,20,21]:
          return "evening"
      else: return "midnight"
  # customer location grouping
  def indo_part(part):
      if part in west_indo:
          return "west_indo"
      elif part in middle_indo:
          return "mid_indo"
      elif part in east_indo:
          return "east_indo"

  # grouping customer age
  data = get_num_people_by_age_category(data)
  data['age_group'] = data['age_group'].astype('str')
  # grouping promo
  data['promo_tipe'] = data['promo_amount'].apply(promo_amount)
  # grouping wilayah
  data['wilayah'] = data['cust_location'].apply(indo_part)
  data['is_jawabali'] = data['cust_location'].apply(lambda x : 1 if x in jawa_bali else 0)
  # grouping time
  data['dayname'] = data.created_at.dt.day_name()
  data['hour'] = data.created_at.dt.hour
  data['daypart'] = data['hour'].apply(daypart)
  data['is_weekend'] = data['dayname'].apply(lambda x : 1 if x in ['Saturday','Sunday'] else 0)
  data['item_price_type'] = data['item_price'].apply(itemprice_type)
  data['quantity_type'] = data['quantity'].apply(quantity_type)
  data['shipment_fee_type'] = data['shipment_fee'].apply(shipment_fee)
  data['cust_jointime_type'] = data['cust_join_time'].apply(cust_jointime)
  data['total_amount'] = (data['item_price'] * data['quantity']) + data['shipment_fee'] - data['promo_amount']
  data['cust_join_time'] = data['cust_join_time'].dt.days
  data['shipment_date_limit'] = pd.to_datetime(data['shipment_date_limit']).dt.tz_localize(None)
  data['shipment_length_limit'] = (data['shipment_date_limit'] - data['created_at']).dt.days
  data['payment_method'] = data['payment_method'].replace({'Debit Card':'debit_card',
                                                                      'OVO':'ovo',
                                                                      'Gopay':'gopay',
                                                                      'Credit Card':'credit_card',
                                                                      'LinkAja':'link_aja'})

  data = data.sort_values(by='created_at')
  return data

data = feature_engineering1(data)
data.info()

"""### Customer Segmentation"""

segmentation = data[['created_at', 'customer_id', 'booking_id', 'payment_method',
                   'payment_status', 'promo_amount', 'shipment_fee', 'shipment_date_limit',
                   'total_amount', 'product_id', 'quantity', 'item_price', 'cust_gender',
                   'device_type', 'cust_age', 'cust_full_name', 'shipment_length_limit',
                   'cust_join_time']
                  ]
segmentation.head()

segmentation.info()

segmentation = segmentation.sort_values(by=['created_at'])

segmentation = segmentation.drop_duplicates()

segmentation.isna().any()

segmentation.info()

"""#### Computing Recency, Frequency, and Monetary Features

* Customer Segmentation adalah proses membagi pelanggan ke dalam kelompok yang berbeda berdasarkan karakteristik umum. Dalam konteks proyek ini, kami mengelompokkan pelanggan melalui Recency, Frequency, Monetary, dan ada tambahan yakni Transaction Age. RFM diklasifikasikan sebagai "behaviour segmentation" (karena mengidentifikasi kecenderungan dan tindakan serta kebiasaan pelanggan yang sering dilakukan) dan "value based segmentation" (karena pelanggan dibagi berdasarkan nilai ekonomi yang mereka berikan kepada bisnis).


* Berdasarkan yang terakhir, kami mengelompokkan pelanggan melalui analisis RFM untuk memahami kemungkinan pelanggan churn serta menemukan segmen pelanggan laten sambil memperkuat pemahaman pelanggan kami dalam bertransaksi.

##### Recency (R)

Fitur recency mengacu pada jumlah hari sejak pembelian terakhir. Untuk menghitung recency, pertama-tama kita perlu menentukan tanggal referensi untuk analisis kita. Oleh karena itu, kami memeriksa tanggal transaksi maksimum dan minimum karena kami akan menggunakan tanggal transaksi terakhir untuk menghitung recency
"""

# Check maximum and minimum dates
print("Min Date :", segmentation["created_at"].dt.date.min(), "Max Date :", segmentation["created_at"].dt.date.max())

"""Menetapkan tanggal terakhir Variabel ke tanggal transaksi terbaru dalam kumpulan data. 31 Juli 2022 adalah tanggal referensi kami"""

lastdate = segmentation["created_at"].dt.date.max()
print(lastdate)

"""Group by customers and check last date of purchase"""

recency = segmentation.groupby(by='customer_id', as_index=False)['created_at'].max()
recency.columns = ['customer_id','LastPurchaseDate']
recency

"""Selanjutnya, kami membuat fitur recency sebagai jumlah hari sebelum tanggal referensi saat pelanggan terakhir melakukan pembelian."""

recency['recency'] = recency['LastPurchaseDate'].dt.date.apply(lambda x: (lastdate - x).days)
recency.head()

print('Min Recency is:', recency['recency'].min())
print('Max Recency is:', recency['recency'].max())

"""Kami mengamati bahwa recency minimum customer telah membeli produk dari aplikasi adalah 0 hari, dan maksimumnya adalah 2221 hari. Analisis akan didasarkan pada recency untuk mensegmentasikan pelanggan dan mengklasifikasikannya sesuai untuk lebih memahami kemungkinan churn mereka.

##### Frequency (F)

Frekuensi memungkinkan kami mengetahui berapa kali pelanggan membeli dari aplikasi. Oleh karena itu, kami perlu memverifikasi berapa banyak transaksi yang didaftarkan oleh pelanggan yang sama.
"""

frequency = segmentation.groupby(by="customer_id",as_index=False).agg({"created_at":"nunique"})
frequency.columns = ['customer_id','frequency']
frequency

print('Min Frequency is:', frequency['frequency'].min())
print('Max Frequency is:', frequency['frequency'].max())

"""Maksimal order per customer adalah 548 order dan minimal 1 order. Dalam analisis RFM kami, kami akan menggunakan informasi pelanggan tentang seberapa sering mereka membeli untuk memahami seberapa aktif mereka.

##### Monetary (M)

Variabel fitur monetary menjelaskan berapa banyak uang yang dihabiskan pelanggan di aplikasi.
"""

#Compute Monetary Values
monetary = segmentation.groupby(by='customer_id',as_index=False).agg({'total_amount': 'sum'})
monetary.columns = ['customer_id','monetary']
monetary

print('Min Monetary is:', monetary['monetary'].min())
print('Max Monetary is:', monetary['monetary'].max())

"""Maksimum uang yang dibelanjakan pelanggan di aplikasi kami adalah Rp320.512.655, sedangkan minimum adalah Rp27.450. Ini akan memungkinkan kami untuk mengelompokkan pelanggan sesuai dengan bagian selanjutnya.

##### Transaction Age (A)

Transaction Age adalah berapa lama waktu antara transaksi pertama pelanggan dengan transaksi terakhirnya.
"""

trans_age = segmentation.groupby(by='customer_id',as_index=False).agg({'created_at': ['min', 'max']})
trans_age.columns = ['customer_id','first_trans', 'last_trans']
trans_age['trans_age'] = (trans_age.last_trans - trans_age.first_trans).dt.days + 1
trans_age

"""##### RFMA Table
Sebagai langkah selanjutnya, kami membuat tabel di mana kami dapat memvisualisasikan nilai individual untuk recency, frequency, dan monetary per pelanggan.
"""

# Drop last Purchase date from recency dataframe as we do not need it anymore.
recency.drop('LastPurchaseDate',axis=1,inplace=True)
trans_age.drop(['first_trans', 'last_trans'],axis=1,inplace=True)
#Merge recency, frequency and monetary dataframes
temp = recency.merge(frequency,on='customer_id')
rfma = temp.merge(monetary,on='customer_id')
rfma = rfma.merge(trans_age,on='customer_id')
rfma

"""#### Encoding
Encoding kami lakukan untuk menghitung aggregat dari tiap nilai unik pada fitur

##### payment_method
"""

segmentation.payment_method.unique()

pay_method = segmentation.loc[:, ['customer_id', 'payment_method']]
dum = pd.get_dummies(pay_method.payment_method, prefix='payment')
pay_method = pd.concat([pay_method, dum], axis=1)
pay_method.drop(columns=['payment_method'], inplace=True)

pay_method = pay_method.groupby(by='customer_id',as_index=False).agg({'payment_debit_card': ['sum'], 'payment_credit_card': ['sum'], 'payment_ovo': ['sum'], 'payment_gopay':['sum'], 'payment_link_aja':['sum']})
pay_method.columns = ['customer_id','payment_debit_card', 'payment_credit_card', 'payment_ovo', 'payment_gopay', 'payment_link_aja']

pay_method

"""##### payment_status"""

segmentation.payment_status.unique()

pay_status = segmentation.loc[:, ['customer_id', 'payment_status']]
dum = pd.get_dummies(pay_status.payment_status, prefix='payment')
pay_status = pd.concat([pay_status, dum], axis=1)
pay_status.drop(columns=['payment_status'], inplace=True)

pay_status = pay_status.groupby(by='customer_id',as_index=False).agg({'payment_Failed': ['sum'], 'payment_Success': ['sum']})
pay_status.columns = ['customer_id','payment_Failed', 'payment_Success']

pay_status

"""##### cust_gender"""

cust_gender = segmentation.loc[:, ['customer_id', 'cust_gender']]
dum = pd.get_dummies(cust_gender.cust_gender, prefix='is')
cust_gender = pd.concat([cust_gender, dum], axis=1)
cust_gender.drop(columns=['cust_gender'], inplace=True)

cust_gender = cust_gender.groupby(by='customer_id',as_index=False).agg({'is_M': ['mean'], 'is_F': ['mean']})
cust_gender.columns = ['customer_id','is_Male', 'is_Female']

cust_gender

"""##### device_type"""

segmentation.device_type.unique()

device_type = segmentation.loc[:, ['customer_id', 'device_type']]
dum = pd.get_dummies(device_type.device_type, prefix='is')
device_type = pd.concat([device_type, dum], axis=1)
device_type.drop(columns=['device_type'], inplace=True)

device_type = device_type.groupby(by='customer_id',as_index=False).agg({'is_Android': ['mean'], 'is_iOS': ['mean']})
device_type.columns = ['customer_id','is_Android', 'is_iOS']

device_type

"""#### Final Segment"""

segment = segmentation.copy()
segment.drop(columns=['created_at', 'booking_id', 'payment_method',
                      'payment_status', 'shipment_date_limit',
                      'product_id', 'cust_gender', 'device_type',
                      ],
             inplace=True)

grouped = segment.groupby(by='customer_id',as_index=False).agg(
    {'promo_amount': ['mean'],
     'shipment_fee': ['mean'],
     'total_amount': ['mean'],
     'quantity':['sum'],
     'item_price': ['mean'],
     'cust_age': ['mean'],
     'cust_join_time': ['mean'],
     'shipment_length_limit': ['mean']
     })
grouped.columns = ['customer_id','avg_promo_amount', 'avg_shipment_fee',
                   'avg_total_amount', 'prod_quantity', 'avg_item_price',
                   'cust_age', 'cust_join_time', 'avg_shipment_length_limit'
                   ]
grouped

#Merge grouped, rfma and payment_type encoding
final_segment = grouped.merge(rfma,on='customer_id')
final_segment = final_segment.merge(pay_method,on='customer_id')
final_segment = final_segment.merge(pay_status,on='customer_id')
final_segment = final_segment.merge(cust_gender,on='customer_id')
final_segment = final_segment.merge(device_type,on='customer_id')

final_segment

"""#### RFM Analysis

"RFM" adalah singkatan dari Recency, Frequency dan Monetary. Analisis RFM adalah cara menggunakan data berdasarkan perilaku pelanggan yang ada untuk memprediksi bagaimana kemungkinan tindakan pelanggan baru di masa depan. Model RFM dibangun menggunakan tiga faktor utama:

- seberapa baru pelanggan bertransaksi (Recency)
- seberapa sering mereka melakukan transaksi (Frequency)
- berapa banyak uang yang mereka habiskan untuk produk dan layanan merek (Monetary)

**Recency**: Ini mengacu pada jumlah waktu sejak transaksi terakhir pelanggan, yang dapat mencakup pembelian terakhir mereka, kunjungan ke situs web, penggunaan aplikasi seluler. Recency adalah metrik utama karena pelanggan yang baru-baru ini melakukan transaksi akan lebih cenderung merespons upaya pemasaran baru.

**Frequency**: Ini mengacu pada berapa kali pelanggan melakukan pembelian atau berinteraksi dengan merek Anda selama periode waktu tertentu. Frekuensi adalah metrik utama karena menunjukkan seberapa dalam pelanggan terlibat dengan merek Anda. Frekuensi yang lebih besar menunjukkan tingkat loyalitas pelanggan yang lebih tinggi.

**Monetary**: Ini mengacu pada jumlah total yang telah dihabiskan pelanggan untuk membeli produk dan layanan dari merek Anda selama periode waktu tertentu. Nilai moneter adalah metrik utama karena pelanggan yang telah menghabiskan paling banyak di masa lalu lebih cenderung membelanjakan lebih banyak di masa depan.

Kita sebelumnya sudah menghitung nilai RFM untuk masing-masing customer, sekarang saatnya melakukan analisis lebih lanjut
"""

RFM_df = final_segment.set_index('customer_id')
RFM_df = RFM_df.loc[:, ['recency', 'frequency', 'monetary']]
RFM_df.describe().T

plt.figure(figsize=(12, 12))
plt.subplot(3, 1, 1); sns.distplot(RFM_df['recency'])
plt.subplot(3, 1, 2); sns.distplot(RFM_df['frequency'])
plt.subplot(3, 1, 3); sns.distplot(RFM_df['monetary'])
plt.show()

RFM_df.info()

"""#### Implementasi RFM Segmentation

Segmentasi pelanggan adalah proses membagi pelanggan ke dalam kelompok yang berbeda berdasarkan karakteristik umum. Dalam konteks proyek ini, kami akan mengelompokkan pelanggan berdasarkan RFM yang sebelumnya telah dibuat. RFM diklasifikasikan sebagai "segmentasi perilaku" (karena mengidentifikasi kecenderungan dan tindakan serta kebiasaan pelanggan yang sering dilakukan) dan "segmentasi berbasis nilai" (karena pelanggan dibagi berdasarkan nilai ekonomi yang mereka berikan kepada bisnis). Segmentasi ini akan membantu perusahaan dalam menentukan strategi/treatment yang tepat berdasarkan segmen ini.

Dalam komputasi, kami akan menerapkan fungsi pada pandas, yakni `pd.qcut` untuk menjadi beberapa kelompok dengan rentang nilai yang sama. Saya membagi setiap variabel baru menjadi "5" gelongan (skor dari 1-5) karena ini adalah standar industri yang menawarkan perincian yang lebih baik. Recency adalah jumlah hari sejak pembelian terakhir. Pelanggan dengan nilai Recency terendah akan mendapat skor tertinggi, karena perusahaan tentu ingin pelanggan kembali lebih sering. Oleh karena itu, semakin rendah skor Recency, semakin tinggi peringkatnya  diantara 1-5. Sebaliknya, pelanggan dengan nilai Frequency dan Monetary tinggi, akan mendapat skor tertinggi antara 1-5, karena pelanggan yang menghabiskan lebih banyak uang dan membeli lebih sering cenderung tidak berhenti bertransaksi.
"""

rfm_segmentation = RFM_df
rfm_segmentation["R"]  = pd.qcut(RFM_df["recency"], 5, labels = [5, 4 , 3, 2, 1])
rfm_segmentation["F"]= pd.qcut(RFM_df["frequency"].rank(method="first"),5, labels=[1,2,3,4,5])
rfm_segmentation["M"] = pd.qcut(RFM_df['monetary'], 5, labels = [1, 2, 3, 4, 5])
rfm_segmentation.head()

rfm_segmentation.dtypes

# Mengubah variabel RFM ke numerik
rfm_segmentation['R'] = rfm_segmentation['R'].astype('int')
rfm_segmentation['F'] = rfm_segmentation['F'].astype('int')
rfm_segmentation['M'] = rfm_segmentation['M'].astype('int')

"""Selanjutnya kita akan nenggabungkan skor RFM individual. Skor RFM akhir dihitung hanya dengan menggabungkan angka skor RFM individual, seperti yang terlihat pada kolom "RFM_Score"
"""

# Memnggabungkan Skor RFM
rfm_segmentation['RFM_Score'] = rfm_segmentation['R'].map(str) + rfm_segmentation['F'].map(str) + rfm_segmentation['M'].map(str)
rfm_segmentation.head()

"""Dengan penggabungan skor RFM, kita akan memiliki 5^3 atau 125 kombinasi skor, yang mana akan terlalu banyak jika kita melakukan segmentasi untuk masing" kombinasi skor. Maka dari itu saya akan menyederhanakan segmentasi, dengan mengelompokkannya menjadi beberapa segmen saja. Untuk mempersimpel komputasi, saya akan menggunakan nilai rata" FM terlebih dahulu.

                                                   FM = (F+M)/2
"""

import math
def truncate(x):
    return math.trunc(x)
rfm_segmentation['FM'] = ((rfm_segmentation['F'] + rfm_segmentation['M'])/2).apply(lambda x: truncate(x))
rfm_segmentation.head()

"""Berdasarkan skor RFM yang akan ditemukan, kita akan tahu apakah pelanggan lebih cenderung churn atau tidak. Saya akan mengkalisifikasikan pelanggan dalam segmen berikut:

* **Champions:** Pelanggan terbaik/impian, karena baru-baru ini membeli dan mentransaksikan nilai dan volume produk tertinggi.


* **Potential Loyalist:** Pelanggan baru dengan nilai transaksi tinggi yang perlu dihargai dan ditreatment dengan baik untuk menjadi pelanggan loyal.


* **New Customer:** Pelanggan yang baru melakukan transaksi, dimana pelanggan ini perlu ditreatment dengan baik agar menjadi pelanggan yang loyal(kembali bertransksi)


* **Loyal Customer:** Pelanggan yang cukup teratur dalam bertransaksi, dengan nilai dan volume yang cukup tinggi.


* **Need Attention:** Pelanggan yang memiliki rata-rata RFM diatas rata", akan tetapi belum melakukan transaksi diakhir" ini.


* **About to Sleep:** Pelanggan lama yang sudah lama tidak melakukan pembelian, bisa hilang jika tidak diaktifkan kembali. Berpotensi Churn.


* **Menjanjikan:** Pelanggan terakhir yang belum banyak bertransaksi.


* **Can't Loose:** Pelanggan bernilai tinggi. Sering melakukan pembelian volume tinggi tetapi tidak kembali dalam waktu yang lama. Berpotensi churn


* **At Risk:** Sering ditransaksikan dengan nilai tinggi, tetapi sudah lama sekali. Karena itu, mereka perlu dibawa kembali. Berpotensi Churn.


* **Hibernation:** Transaksi terakhir sudah lama, frekuensi dan nilai transaksi rendah. Berpotensi Churn.


* **Lost:** Transkasi sejak dahulu kala dan bisa dipastikan tidak pernah kembali (churned).
"""

# RFM Segments
seg_map = {
    r'22': 'Hibernating',
    r'[1-2][1-2]': 'Lost',
    r'15': 'Can\'t Lose',
    r'[1-2][3-5]': 'At Risk',
    r'3[1-2]': 'About to Sleep',
    r'33': 'Need Attention',
    r'55': 'Champions',
    r'[3-5][4-5]': 'Loyal Customers',
    r'41': 'Promising',
    r'51': 'New Customers',
    r'[4-5][2-3]': 'Potential Loyalists'
    }

rfm_segmentation['RFM_Segment'] = rfm_segmentation['R'].astype(str) + rfm_segmentation['FM'].astype(str)
rfm_segmentation['RFM_Segment'] = rfm_segmentation['RFM_Segment'].replace(seg_map, regex=True)
rfm_segmentation

"""Eksplorasi Segment RFM"""

RFMStats = rfm_segmentation[["RFM_Segment", "recency", "frequency", "monetary"]].groupby("RFM_Segment").agg(['mean','median', 'min', 'max', 'count'])
RFMStats['Ratio']= (100*RFMStats['monetary']["count"]/RFMStats['monetary']["count"].sum()).round(2)
RFMStats

"""Seperti yang dapat kita amati dari summary statistic di atas, sebagian customer di Fashion Campus, yakni **25.30%** dari total pelanggan, adalah **"Lost Customers"** yang ditandai dengan skor Recency, Frequency, dan Monetary yang rendah. Pelanggan ini telah churn (churned), oleh karena itu mereka berhenti membeli di aplikasi online dan telah memutuskan untuk menggunakan alternatif lain sebagai gantinya. Tercatat segmen ini memiliki rata-rata resensi 875.19 hari, rata-rata frekuensi 1.41, dan rata-rata monetary   681,592.7

**"Loyal Customers"**, sebanyak 25.24% dari total pelanggan. Pelanggan di segmen ini senang dengan produk dan tidak akan beralih ke alternatif lain. Mereka membeli setiap 56.28 hari dan menempatkan sekitar 29.49 pesanan dengan pengeluaran rata-rata 16,593,480. Penting untuk membuat pelanggan ini merasa dihargai.

**"Potential Loyalist"** terdiri dari 9.87% pelanggan, Pelanggan di segmen ini berpotensi menjadi pelanggan loyal. Mereka memiliki rata-rata resensi 35.84 hari, rata-rata frekuensi 6.55, dan rata-rata monetary 3,211,941.

Pelanggan **"At Risk" (6.53%)** dan **"About to Sleep" (6.31%)**, adalah pelanggan lama yang sudah lama tidak melakukan pembelian dan berada di ambang churn, perusahaan perlu melakukan dorongan terakhir untuk memastikan retensi pelanggan dan membuat pesan yang paling relevan untuk melakukannya. Pelanggan At Risk memiliki rata-rata resensi 269.02 hari, rata-rata frekuensi 6.43, dan rata-rata monetary 4,354,260. Sedangkan pelanggan About to Sleep memiliki rata-rata resensi 121.11 hari, rata-rata frekuensi 2.78, dan rata-rata monetary 1,133,180.

Pelanggan **"Hibernating" (8.16%)** , **"Need Attention" (6.82%)**, dan **"Promising" (0.76%)** ditandai dengan frekuensi rendah dan perlu mendapat perhatian khusus karena mereka lebih cenderung churn. Pelanggan Hibernating memiliki rata-rata resensi 303.352 hari, rata-rata frekuensi 3.27, dan rata-rata monetary 1,490,963. Pelanggan Need Attention memiliki rata-rata resensi 117.77 hari, rata-rata frekuensi 7.87, dan rata-rata monetary 4,009,483. Sedangkan pelanggan Promising memiliki rata-rata resensi 50.87 hari, rata-rata frekuensi 1.36, dan rata-rata monetary 411,660.6

Terakhir, **"Champions"**, lebih dikenal sebagai pelanggan impian, terdiri dari 10,27% dari total pelanggan (sangat sedikit), memiliki skor tinggi di ketiga kategori RFM. Pelanggan ini memiliki rata-rata resensi 13.617 hari, rata-rata frekuensi 67.56, dan rata-rata monetary 37,626,210.
"""

plt.figure(figsize=(18,12))
plt.rc('font', size=15)
squarify.plot(sizes=RFMStats["recency"]["count"], label=RFMStats.index,
              color=["red","orange","blue", "forestgreen", "yellow", "purple", "cornsilk","royalblue", "pink", "brown"], alpha=.85)
plt.suptitle("Recency and Frequency Grid", fontsize=25);

"""#### K-Means Clustering

K-means adalah algoritma unsupervised learning, yang digunakan untuk pengelompokan data. Dalam algoritma k-means, jumlah cluster K telah ditentukan sebelumnya dan algoritma secara iteratif memberikan setiap titik data ke salah satu cluster K berdasarkan kesamaan fitur.

#### Implementasi K Means Clustering

#### Langkah :

* Pre-Processing Data
* Tentukan Jumlah Cluster
* Menjalankan K Berarti Pengelompokan pada Data yang Diproses sebelumnya
* Analisis cluster

#### K-means memberikan hasil terbaik dalam kondisi berikut:

* Distribusi data tidak miring (skew).
* Data distandarisasi (yaitu rata-rata 0 dan standar deviasi 1).

Oleh karena itu, kita akan memeriksa distribusi data untuk setiap fitur yang baru dihitung (RFM).
"""

RFM_df.head()

# RFM Table
rfm_kmeans= rfm_segmentation[['recency','frequency','monetary']]

# Plot RFM Distributions
plt.figure(figsize=(12,12))
plt.subplot(3, 1, 1); sns.histplot(rfm_kmeans['recency'], kde=True,  linewidth=0)# Plot distribution of R
plt.subplot(3, 1, 2); sns.histplot(rfm_kmeans['frequency'], kde=True,  linewidth=0)# Plot distribution of F
plt.subplot(3, 1, 3); sns.histplot(rfm_kmeans['monetary'], kde=True,  linewidth=0)# Plot distribution of M
plt.show()# Show the plot

"""Seperti yang kita amati pada gambar di atas, ketiga variabel, Recency, Frequency, dan Monetary miring ke kanan (right skewed). Oleh karena itu, perlu dilakukan normalisasi data. Tujuan normalisasi adalah untuk mengubah nilai kolom numerik dalam kumpulan data untuk menggunakan skala standar, tanpa mendistorsi perbedaan dalam rentang nilai atau kehilangan informasi.

##### Apply Square Root Transformation

Seperti yang kita amati di atas bahwa distribusi data tidak simetris, maka perlu diterapkan _square root transfromation_ untuk menormalkan skewness.
"""

rfm_sqrt = rfm_kmeans[['recency', 'frequency', 'monetary']].apply(np.sqrt, axis = 1).round(3)


f,ax = plt.subplots(figsize=(15, 15))
plt.subplot(3, 1, 1); sns.distplot(rfm_sqrt.recency, label = 'Recency')
plt.subplot(3, 1, 2); sns.distplot(rfm_sqrt.frequency, label = 'Frequency')
plt.subplot(3, 1, 3); sns.distplot(rfm_sqrt.monetary, label = 'Monetary')
plt.style.use('fivethirtyeight')
plt.tight_layout()
plt.show()

"""##### Preprocessing the Data

Pertama-tama kita perlu menstandarisasi semua parameter
"""

from sklearn.preprocessing import StandardScaler
standard_scaler = StandardScaler()
rfm_norm = standard_scaler.fit_transform(rfm_sqrt)

rfm_norm = pd.DataFrame(rfm_norm)
rfm_norm.columns = ['recency', 'frequency', 'monetary']
rfm_norm.head()

"""##### Determine Optimal Number of Clusters

Langkah selanjutnya, kita akan menggunakan _elbow method_ untuk menentukan nilai k yang optimal. Ketika kita memplot WCSS ( Within-Cluster Sum of Square ), plot tersebut dapat dilihat sebagai siku-siku. Saat menganalisis plot, kita akan melihat bagaimana pada suatu titik grafik dengan cepat kehilangan bentuk sikunya. Pada titik itulah kita dapat menentukan nilai-k.
"""

wcss = []

for i in range (1,10): #15 cluster
    kmeans = KMeans(n_clusters = i, init='k-means++', random_state=0)
    kmeans.fit(rfm_norm)
    wcss.append(kmeans.inertia_)

plt.plot(range(1,10),wcss,  marker='D')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('wcss')
plt.show()

from kneed import KneeLocator
kl = KneeLocator(range(1, 10), wcss, curve="convex", direction="decreasing")
kl.elbow

"""Kita akan menggunakan Nilai K optimal dari hasil kneed, yakni adalah 3"""

# K-means clustering
kc = KMeans(n_clusters=3,random_state=0)
kc.fit(rfm_norm)

"""Fungsi dibawah ini akan digunakan untuk membuat plot 3d yang merepresentasikan cluster."""

def plots_model():
    fig = plt.figure(figsize=(15, 12))
    ax = fig.add_subplot(111, projection='3d')

    for x in rfm_norm.groups.unique():
        xs = rfm_norm[rfm_norm.groups == x]['recency']
        zs = rfm_norm[rfm_norm.groups == x]['frequency']
        ys = rfm_norm[rfm_norm.groups == x]['monetary']
        ax.scatter(xs, ys, zs, s=50, alpha=0.6, edgecolors='w', label = x)

    ax.set_xlabel('Recency')
    ax.set_zlabel('Frequency')
    ax.set_ylabel('Monetary')
    plt.title('Visualization of created groups')
    plt.legend()
    plt.show()

"""##### Visualisasi Cluster"""

model_1 = KMeans(n_clusters=3, init='k-means++', max_iter=300)
groups = model_1.fit_predict(rfm_norm)
rfm_norm['groups'] = groups
plots_model()

rfm_segmentation['K_Cluster'] = kc.labels_
rfm_segmentation

# Function to visualize clusters
def rfm_clusters_stat(df):
    df_new = df.groupby(['K_Cluster']).agg({
            'recency'  : ['mean','median', 'min', 'max'],
            'frequency': ['mean','median', 'min', 'max'],
            'monetary' : ['mean','median', 'min', 'max', 'count']
        }).round(0)

    return df_new

rfm_clusters_stat(rfm_segmentation).style.background_gradient(cmap='YlGnBu')

"""#### Cluster Description

Cluster 0 adalah cluster tengah, dimana cluster ini memiliki skor dan rata" RFM yang bukan terburuk dan bukan terbaik. R = 109, F = 9, M = 4,957,785. Jumlahnya paling banyak
Cluster 1 adalah cluster terburuk, skor RFM cluster ini adalah yang paling jelek. R = 845, F = 2, M = 857,287. Terbanyak kedua
Cluster 2 adalah cluster terbaik, diaman skor RFM mereka tertinggi. Rata" R = 28, F = 63, dan M = 35,652,285. Paling sedikit.

###### Cluster 0
"""

rfm_segmentation[rfm_segmentation.K_Cluster == 0]['RFM_Segment'].value_counts()

"""Cluster 0 adalah customer yang rawan. Hal ini karena, Cluster ini berisi campuran berbagai segment customer. Yang perlu di highlight adalah bahwa cluster ini memiliki 5002 Potential Loyalist, 3460 Need Attention, 3199 About to Sleep, 2978 At Risk, dan 2387 Hibernating. Segmen" ini adalah segmen yang memiliki skor resensi, frekuensi, dan monetary di tengah", bukan terbaik atau terburuk, sehingga cluster ini berpotensi menjadi churn/lost atau bisa juga menjadi loyal. Maka dari itulah mengapa customer ini rawan, dan perlu dilakukan treatment segera. Jumlah customer pada cluster 0 ini adalah yang paling banyak diantara semua cluster, yakni 27561 orang.

###### Cluster 1
"""

rfm_segmentation[rfm_segmentation.K_Cluster == 1]['RFM_Segment'].value_counts()

"""Cluster 1 dapat dikatakan sebagai terburuk. Cluster ini didominasi oleh segmen 12002 segmen Lost, 1750 segmen Hibernating, dan 334 segmen At Risk. Cluster ini memiliki skor resensi, monetary dan frequency terburuk. Cluster ini memiliki 14086 orang.

###### Cluster 2
"""

rfm_segmentation[rfm_segmentation.K_Cluster == 2]['RFM_Segment'].value_counts()

"""Cluster 2 merepresentasikan customer terbaik, yakni pelanggan yang memiliki skor RFM paling tinggi. Cluster ini didominasi oleh Champions (5152) orang, Loyal Customers (3900 orang) dan 1 segmen At Risk. Cluster 1 adalah yang terbaik, karena memiliki rata-rata frekuensi dan monetary tertinggi dari semua cluster. Cluster ini adalah yang perlu dipertahankan. Sayangnya jumlah customer yang masuk pada cluster ini adalah yang paling sedikit, yakni 9053 orang."""

RFM_stats= pd.DataFrame(rfm_clusters_stat(rfm_segmentation))

# Visualize Segments
plt.figure(figsize=(12,8))
squarify.plot(sizes=RFM_stats["monetary"]["count"], label=RFM_stats.index, color=["y", "r", "g"], alpha=0.7)
plt.suptitle("Segments of Customers", fontsize=25)

rfm_cluster = rfm_segmentation.loc[:, ['R', 'F', 'M', 'K_Cluster']]
rfm_cluster

rfm_cluster = rfm_cluster.reset_index(drop=False)
rfm_cluster

cust_segment_table = rfm_cluster.merge(final_segment, on='customer_id')

cust_segment_table.shape

cust_segment_table.duplicated().sum()

cust_segment_table.head()

cust_RFMC = cust_segment_table.loc[:, ['customer_id', 'R', 'F', 'M', 'K_Cluster']].copy()

# cust_segment_table.to_csv('/content/gdrive/MyDrive/FINAL PROJECT BULGARIA/DATASET/cust_segment_table.csv', index=False)

"""### Churn Label

#### RFM Version

Definisi Versi RFM: <br>
Customer yang tergolong churn adalah customer yang tidak masuk ke cluster 1, atau customer yang masuk ke cluster 2 dengan skor R yang kurang dari 4. Selain itu, maka customer tidak tergolong churn
"""

def is_churn(cluster, R):
  if(cluster != 1):
    if(cluster == 2):
      if(R > 3):
        return 0
      else:
        return 1
      return 1
    return 1
  else:
    return 0

rfm_model = cust_segment_table.copy()
rfm_model['is_churn'] = rfm_model.apply(lambda x: is_churn(x['K_Cluster'], x['R']), axis=1)

print('Recency Min : ', cust_segment_table.recency.min())
print('Recency Max : ', cust_segment_table.recency.max())

rfm_model.is_churn.value_counts()

rfm_model

"""#### Windowing Version

Untuk definisi churn menggunakan windowing version, kami akan membagi dataset menjadi dua bagian berdasarkan callibration period dan holdout period. Callibration Period adalah periode waktu yang kami gunakan untuk menggenerate feature. Versi callibration period yang akan kami gunakan adalah 6 bulan, 3 bulan, 1 bulan, dan 1 minggu. Sedangkan untuk holdout period, akan kami gunakan periode satu bulan tepat setelah callibration period. Holdout Period ini kami gunakan untuk memberikan label pada customer apakah churn atau tidak. Definisi dari customer churn adalah mereka yang bertransaksi di callibration period, namun tidak bertransaksi di holdout period. Customer yang terdefinisi sebagai churn akan kami beri nilai 1 (True), jika tidak churn maka nilainya 0 (False)

##### Defining Function for Labeling
"""

from dateutil import rrule
# Labeling Function for Specific Interval
def disc_label(data, start_date, freq, interval, new_data):
  if freq == 'months':
    call = (data['created_at'] >= start_date) & (data['created_at'] < (start_date + pd.DateOffset(months=interval)))
    hold = (data['created_at'] >= (start_date + pd.DateOffset(months=interval))) & (data['created_at'] < (start_date + pd.DateOffset(months=interval+1)))
  if freq == 'days':
    call = (data['created_at'] >= start_date) & (data['created_at'] < (start_date + pd.DateOffset(days=interval)))
    hold = (data['created_at'] >= (start_date + pd.DateOffset(days=interval))) & (data['created_at'] < (start_date + pd.DateOffset(days=interval) + pd.DateOffset(months=1)))
  feat = data.loc[call].copy()
  lab = data.loc[hold].copy()
  lab['is_churn'] = 0
  lab = lab.loc[:, ['customer_id', 'is_churn']]
  lab = lab.drop_duplicates()
  new_data = feat.merge(lab, on='customer_id', how='left')
  new_data['is_churn'] = new_data['is_churn'].fillna(1)
  new_data = new_data.sort_values(by='created_at')
  return new_data

# Labeling Function for All Data
def all_label(data, freq, interval):
  if freq == 'months':
    databaru = pd.DataFrame([])
    for i in rrule.rrule(rrule.MONTHLY, dtstart=datetime(2016, 7, 1), until=(data.created_at.max().replace(day=1) - pd.DateOffset(months=interval))):
      print('Calculating Date: ', i)
      def calc(data_satuan=data):
        call = (data_satuan['created_at'] >= i) & (data_satuan['created_at'] < (i + pd.DateOffset(months=interval)))
        hold = (data_satuan['created_at'] >= (i + pd.DateOffset(months=interval))) & (data_satuan['created_at'] < (i + pd.DateOffset(months=interval+1)))
        feat = data_satuan.loc[call].copy()
        lab = data_satuan.loc[hold].copy()
        lab['is_churn'] = 0
        lab = lab.loc[:, ['customer_id', 'is_churn']]
        lab = lab.drop_duplicates()
        anyar = feat.merge(lab, on='customer_id', how='left')
        anyar['is_churn'] = anyar['is_churn'].fillna(1)
        anyar = anyar.sort_values(by='created_at')
        return anyar
      databaru = databaru.append(calc())
    return databaru
  if freq == 'weeks':
    databaru = pd.DataFrame([])
    for i in rrule.rrule(rrule.WEEKLY, dtstart=datetime(2016, 7, 1), until=(data.created_at.max().replace(day=1) - pd.DateOffset(weeks=interval))):
      print('Calculating Date: ', i)
      def calc(data_satuan=data):
        call = (data_satuan['created_at'] >= i) & (data_satuan['created_at'] < (i + pd.DateOffset(weeks=interval)))
        hold = (data_satuan['created_at'] >= (i + pd.DateOffset(weeks=interval))) & (data_satuan['created_at'] < (i + pd.DateOffset(weeks=interval) + pd.DateOffset(months=1)))

        feat = data_satuan.loc[call].copy()
        lab = data_satuan.loc[hold].copy()
        lab['is_churn'] = 0
        lab = lab.loc[:, ['customer_id', 'is_churn']]
        lab = lab.drop_duplicates()
        anyar = feat.merge(lab, on='customer_id', how='left')
        anyar['is_churn'] = anyar['is_churn'].fillna(1)
        anyar = anyar.sort_values(by='created_at')
        return anyar
      databaru = databaru.append(calc())
    return databaru

"""##### Churn Label for All Data"""

churn_all_m = None
churn_all_m = all_label(data, 'months', 1)

churn_all_w = None
churn_all_w = all_label(data, 'weeks', 1)

print(data.created_at.max())
print('Churn All Data Monthly Callibration')
print(churn_all_m.created_at.max())
print(churn_all_m.is_churn.value_counts())
print('Churn All Data Weekly Callibration')
print(churn_all_w.created_at.max())
print(churn_all_w.is_churn.value_counts())

# exporting dataset
# churn_all_w_RFMC = churn_all_w.merge(cust_RFMC, on='customer_id', how='left')
# churn_all_m_RFMC = churn_all_m.merge(cust_RFMC, on='customer_id', how='left')
# churn_all_w_RFMC.to_csv(path+'churn_data_weekly.csv', index=False)
# churn_all_m_RFMC.to_csv(path+'churn_data_monthly.csv', index=False)

"""##### Churn Label for Specific Data

###### Callibration Periods

6 months <br>
---
feature = 1 Januari 2022 - 30 Juni 2022 <br>
label = 1 Juli 2022 - 31 Juli 2022
3 months
---
feature = 1 April 2022 - 30 Juni 2022 <br>
label = 1 Juli 2022 - 31 Juli 2022
1 months
---
feature = 1 Juni 2022 - 30 Juni 2022 <br>
label = 1 Juli 2022 - 31 Juli 2022
7 days
---
feature = 24 Juni 2022 - 30 Juni 2022 <br>
label = 1 Juli 2022 - 31 Juli 2022

###### Calculating
"""

churn_6m = None
churn_3m = None
churn_1m = None
churn_7d = None

churn_6m = disc_label(data, datetime(2022, 1, 1), 'months', 6, churn_6m)
churn_3m = disc_label(data, datetime(2022, 4, 1), 'months', 3, churn_3m)
churn_1m = disc_label(data, datetime(2022, 6, 1), 'months', 1, churn_1m)
churn_7d = disc_label(data, datetime(2022, 6, 24), 'days', 7, churn_7d)

data_cal = [churn_6m, churn_3m, churn_1m, churn_7d]
for i in data_cal:
  print(i.is_churn.value_counts())
  churn_rate = (sum(i['is_churn'])/len(i['is_churn'].index))*100
  print('Churn Rate ' + ' : ' + str(churn_rate))

# # exporting dataset
# churn_6m.to_csv(path+'churn_data_6_month.csv', index=False)
# churn_3m.to_csv(path+'churn_data_3_month.csv', index=False)
# churn_1m.to_csv(path+'churn_data_1_month.csv', index=False)
# churn_7d.to_csv(path+'churn_data_7_days.csv', index=False)

"""#### Attribute Selection (Windowing)"""

# All Data
churn_all_m_fs = churn_all_m[['payment_method', 'payment_status',
                        'cust_gender', 'device_type', 'age_group', 'promo_tipe', 'wilayah',
                        'dayname', 'daypart', 'total_amount', 'is_jawabali', 'is_weekend',
                        'shipment_length_limit', 'item_price_type', 'quantity_type',
                        'shipment_fee_type', 'cust_jointime_type', 'is_churn'
                        ]].copy()
churn_all_w_fs = churn_all_w[['payment_method', 'payment_status',
                        'cust_gender', 'device_type', 'age_group', 'promo_tipe', 'wilayah',
                        'dayname', 'daypart', 'total_amount', 'is_jawabali', 'is_weekend',
                        'shipment_length_limit', 'item_price_type', 'quantity_type',
                        'shipment_fee_type', 'cust_jointime_type', 'is_churn'
                        ]].copy()
# Specific Data
churn_6m_fs = churn_6m[['payment_method', 'payment_status',
                        'cust_gender', 'device_type', 'age_group', 'promo_tipe', 'wilayah',
                        'dayname', 'daypart', 'total_amount', 'is_jawabali', 'is_weekend',
                        'shipment_length_limit', 'item_price_type', 'quantity_type',
                        'shipment_fee_type', 'cust_jointime_type', 'is_churn'
                        ]].copy()
churn_3m_fs = churn_3m[['payment_method', 'payment_status',
                        'cust_gender', 'device_type', 'age_group', 'promo_tipe', 'wilayah',
                        'dayname', 'daypart', 'total_amount', 'is_jawabali', 'is_weekend',
                        'shipment_length_limit', 'item_price_type', 'quantity_type',
                        'shipment_fee_type', 'cust_jointime_type', 'is_churn'
                        ]].copy()
churn_1m_fs = churn_1m[['payment_method', 'payment_status',
                        'cust_gender', 'device_type', 'age_group', 'promo_tipe', 'wilayah',
                        'dayname', 'daypart', 'total_amount', 'is_jawabali', 'is_weekend',
                        'shipment_length_limit', 'item_price_type', 'quantity_type',
                        'shipment_fee_type', 'cust_jointime_type', 'is_churn'
                        ]].copy()
churn_7d_fs = churn_7d[['payment_method', 'payment_status',
                        'cust_gender', 'device_type', 'age_group', 'promo_tipe', 'wilayah',
                        'dayname', 'daypart', 'total_amount', 'is_jawabali', 'is_weekend',
                        'shipment_length_limit', 'item_price_type', 'quantity_type',
                        'shipment_fee_type', 'cust_jointime_type', 'is_churn'
                        ]].copy()

# All Data
churn_all_m_fs = churn_all_m_fs.drop_duplicates()
churn_all_w_fs = churn_all_w_fs.drop_duplicates()
# Specific Data
churn_6m_fs = churn_6m_fs.drop_duplicates()
churn_3m_fs = churn_3m_fs.drop_duplicates()
churn_1m_fs = churn_1m_fs.drop_duplicates()
churn_7d_fs = churn_7d_fs.drop_duplicates()

data_all_fs = [churn_all_m_fs, churn_all_w_fs]
data_spec_fs = [churn_6m_fs, churn_3m_fs, churn_1m_fs, churn_7d_fs]

"""#### Square Root Transformation

##### All Data
"""

for i in data_all_fs:
  f,ax = plt.subplots(figsize=(8, 6))
  sns.histplot(i['total_amount'], label = 'Total Amount', kde=True)
  plt.style.use('fivethirtyeight')
  plt.tight_layout()
  plt.show()

churn_all_m_fs_sqrt = churn_all_m_fs.copy()
churn_all_w_fs_sqrt = churn_all_m_fs.copy()

datalist_all_sqrt = [churn_all_m_fs_sqrt, churn_all_w_fs_sqrt]
for i in datalist_all_sqrt:
  i['total_amount'] = i['total_amount'].apply(np.sqrt, axis = 1).round(3)
  f,ax = plt.subplots(figsize=(8, 6))
  sns.distplot(i['total_amount'], label = 'Total Amount')
  plt.style.use('fivethirtyeight')
  plt.tight_layout()
  plt.show()

"""##### Specific Data"""

for i in data_spec_fs:
  f,ax = plt.subplots(figsize=(8, 6))
  sns.histplot(i['total_amount'], label = 'Total Amount', kde=True)
  plt.style.use('fivethirtyeight')
  plt.tight_layout()
  plt.show()

churn_6m_fs_sqrt = churn_6m_fs.copy()
churn_3m_fs_sqrt = churn_3m_fs.copy()
churn_1m_fs_sqrt = churn_1m_fs.copy()
churn_7d_fs_sqrt = churn_7d_fs.copy()

datalist_spec_sqrt = [churn_6m_fs_sqrt, churn_3m_fs_sqrt, churn_1m_fs_sqrt, churn_7d_fs_sqrt]
for i in datalist_spec_sqrt:
  i['total_amount'] = i['total_amount'].apply(np.sqrt, axis = 1).round(3)
  f,ax = plt.subplots(figsize=(8, 6))
  sns.distplot(i['total_amount'], label = 'Total Amount')
  plt.style.use('fivethirtyeight')
  plt.tight_layout()
  plt.show()

"""#### Standardization"""

# Standardization

# All Data
churn_all_m_fs_std = churn_all_m_fs_sqrt.copy()
churn_all_w_fs_std = churn_all_w_fs_sqrt.copy()
datalist_all_std = [churn_all_m_fs_std, churn_all_w_fs_std]

for i in datalist_all_std:
  total_amount_mean = i.total_amount.mean()
  total_amount_std = i.total_amount.std()
  i['total_amount'] = i['total_amount'].apply(lambda x: (x - total_amount_mean) / total_amount_std)

# Specific Data
churn_6m_fs_std = churn_6m_fs_sqrt.copy()
churn_3m_fs_std = churn_3m_fs_sqrt.copy()
churn_1m_fs_std = churn_1m_fs_sqrt.copy()
churn_7d_fs_std = churn_7d_fs_sqrt.copy()
datalist_spec_std = [churn_6m_fs_std, churn_3m_fs_std, churn_1m_fs_std, churn_7d_fs_std]
for i in datalist_spec_std:
  total_amount_mean = i.total_amount.mean()
  total_amount_std = i.total_amount.std()
  i['total_amount'] = i['total_amount'].apply(lambda x: (x - total_amount_mean) / total_amount_std)

"""#### Encoding"""

# All Data
churn_all_m_fs_encode = pd.get_dummies(churn_all_m_fs_std, columns=churn_all_m_fs_std.select_dtypes(include='object').columns)
churn_all_w_fs_encode = pd.get_dummies(churn_all_w_fs_std, columns=churn_all_w_fs_std.select_dtypes(include='object').columns)
# Specific Data
churn_6m_fs_encode = pd.get_dummies(churn_6m_fs_std, columns=churn_6m_fs_std.select_dtypes(include='object').columns)
churn_3m_fs_encode = pd.get_dummies(churn_3m_fs_std, columns=churn_3m_fs_std.select_dtypes(include='object').columns)
churn_1m_fs_encode = pd.get_dummies(churn_1m_fs_std, columns=churn_1m_fs_std.select_dtypes(include='object').columns)
churn_7d_fs_encode = pd.get_dummies(churn_7d_fs_std, columns=churn_7d_fs_std.select_dtypes(include='object').columns)

churn_all_encode = [churn_all_m_fs_encode, churn_all_w_fs_encode]
churn_spec_encode = [churn_6m_fs_encode, churn_3m_fs_encode, churn_1m_fs_encode, churn_7d_fs_encode]

"""## Modeling"""

def cross_validation(model,xtrain,ytrain, scoretype, folds):
    scores = cross_val_score(estimator = model,X= xtrain, y = ytrain,scoring = scoretype,cv = folds)
    print("%s: %0.3f (+/- %0.2f)" % ("roc-auc",scores.mean(),scores.std()))

def roc_curve(X_test, y_test, model, model_name):
  y_pred_proba = model.predict_proba(X_test)[::,1]
  fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
  auc = metrics.roc_auc_score(y_test, y_pred_proba)
  #create ROC curve
  plt.figure(figsize=(8,8))
  plt.title(model_name, fontsize=16)
  plt.plot(fpr,tpr,label="Model:" + model_name+"(AUC="+str(auc)+")")
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.legend(loc=4)
  plt.show()

"""#### Baselines

##### Baseline Function
"""

period = ['6 months', '3 months', '1 months', '7 days']
all_period = ['Monthly', 'Weekly']
def baseline_score(data, train_size, test_size, range):
  # split
  X = data.drop(['is_churn'], axis=1)
  y = data[['is_churn']]
  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, test_size=test_size, random_state=100)

  algorithm = []
  accuracy = []
  precision = []
  recall = []
  f1 = []
  f2 = []
  roc_auc = []

  # Logistic Regression
  logreg = LogisticRegression(max_iter=10000)
  logreg.fit(X_train, y_train.values.ravel())
  y_predict = logreg.predict(X_test)
  print('\n')
  print('Logistic Regression')
  print(classification_report(y_test, y_predict))

  print("Accuracy :", logreg.score(X_test, y_test))
  print("ROC_AUC :", roc_auc_score(y_test, y_predict))

  lr_accuracy = str(accuracy_score(y_test, y_predict))
  lr_precision = str(precision_score(y_test, y_predict))
  lr_recall = str(recall_score(y_test, y_predict))
  lr_f1 = str(f1_score(y_test, y_predict))
  lr_f2 = str(fbeta_score(y_test, y_predict, beta=2.0))
  lr_rocauc = str(roc_auc_score(y_test, y_predict))

  algorithm.append('Logistic Regression')
  accuracy.append(lr_accuracy)
  precision.append(lr_precision)
  recall.append(lr_recall)
  f1.append(lr_f1)
  f2.append(lr_f2)
  roc_auc.append(lr_rocauc)


  # Decision Tree
  dtree = DecisionTreeClassifier(random_state = 42)
  dtree.fit(X_train,y_train.values.ravel())
  y_predict = dtree.predict(X_test)
  print('\n')
  print('Decision Tree')
  print(classification_report(y_test, y_predict))


  # dt_accuracy = str(dtree.score(X_test, y_test))
  # dt_rocauc = str(roc_auc_score(y_test, y_predict))
  # algorithm.append('Decision Tree')
  # accuracy.append(dt_accuracy)
  # roc_auc.append(dt_rocauc)

  dt_accuracy = str(accuracy_score(y_test, y_predict))
  dt_precision = str(precision_score(y_test, y_predict))
  dt_recall = str(recall_score(y_test, y_predict))
  dt_f1 = str(f1_score(y_test, y_predict))
  dt_f2 = str(fbeta_score(y_test, y_predict, beta=2.0))
  dt_rocauc = str(roc_auc_score(y_test, y_predict))

  algorithm.append('Decision Tree')
  accuracy.append(dt_accuracy)
  precision.append(dt_precision)
  recall.append(dt_recall)
  f1.append(dt_f1)
  f2.append(dt_f2)
  roc_auc.append(dt_rocauc)

  print("Accuracy:",dtree.score(X_test, y_test))
  print("ROC_AUC :", roc_auc_score(y_test, y_predict))

  # Random Forest
  rf = RandomForestClassifier()
  rf.fit(X_train,y_train.values.ravel())
  y_predict = rf.predict(X_test)
  print('\n')
  print('Random Forest')
  print(classification_report(y_test, y_predict))

  # rf_accuracy = str(rf.score(X_test, y_test))
  # rf_rocauc = str(roc_auc_score(y_test, y_predict))
  # algorithm.append('Random Forest')
  # accuracy.append(rf_accuracy)
  # roc_auc.append(rf_rocauc)

  rf_accuracy = str(accuracy_score(y_test, y_predict))
  rf_precision = str(precision_score(y_test, y_predict))
  rf_recall = str(recall_score(y_test, y_predict))
  rf_f1 = str(f1_score(y_test, y_predict))
  rf_f2 = str(fbeta_score(y_test, y_predict, beta=2.0))
  rf_rocauc = str(roc_auc_score(y_test, y_predict))

  algorithm.append('Random Forest')
  accuracy.append(rf_accuracy)
  precision.append(rf_precision)
  recall.append(rf_recall)
  f1.append(rf_f1)
  f2.append(rf_f2)
  roc_auc.append(rf_rocauc)

  print("Accuracy :", rf.score(X_test, y_test))
  print("ROC_AUC :", roc_auc_score(y_test, y_predict))

  if range == 'specific':
    d = {"Period": period[j], "Algorithm": algorithm, "Accuracy": accuracy, "Precision": precision, "Recall": recall,
         "F1 Score": f1, "F2 Score": f2, "ROC-AUC": roc_auc}
  if range == 'all':
    d = {"Period": period[j], "Algorithm": algorithm, "Accuracy": accuracy, "Precision": precision, "Recall": recall,
         "F1 Score": f1, "F2 Score": f2, "ROC-AUC": roc_auc}
  scores = pd.DataFrame(d)
  return scores

"""##### Long Date Baseline"""

# j = 0
# score = pd.DataFrame([])
# print(all_period[1])
# score = score.append(baseline_score(i, 0.8, 0.2, 'all'))
# j+=1
# score

"""##### Specific Date Baseline"""

j = 0
score = pd.DataFrame([])
for i in churn_spec_encode:
  print(period[j])
  score = score.append(baseline_score(i, 0.8, 0.2, 'specific'))
  j+=1
score

j = 0
score = pd.DataFrame([])
for i in churn_spec_encode:
  print(period[j])
  score = score.append(baseline_score(i, 0.8, 0.2, 'specific'))
  j+=1
score

index = pd.Index(list(range(0, len(score))))
score = score.set_index(index)
score.sort_values(by=["Precision", "Recall", "F2 Score"], ascending=False)

"""Berdasarkan performa model, maka akan dipilih weekly (7 days) sebagai callibration period untuk churn prediction

#### EDA Phase 2

##### Churn Weekly Long Date
"""

churn_all_w_eda = churn_all_w.drop(columns=['promo_code', 'shipment_location_lat',
                                            'shipment_location_long', 'product_id',
                                            'cust_first_name', 'cust_last_name',
                                            'username', 'email', 'birthdate',
                                            'device_id', 'device_version', 'cust_location_lat',
                                            'cust_location_long', 'cust_country',
                                            'cust_join_date', 'join_year',
                                            'join_month', 'join_dayname', 'cust_full_name',
                                            'id', 'prodMasterCategory', 'prodSubCategory',
                                            'prodArticleType', 'prodBaseColour',
                                            'prodSeason', 'prod_year', 'prod_usage',
                                            'productDisplayName'])

def to_string(data):
  if data == 1:
    return 'Yes'
  else:
    return 'No'

churn_all_w_eda['is_jawabali'] = churn_all_w_eda['is_jawabali'].apply(to_string)
churn_all_w_eda['is_weekend'] = churn_all_w_eda['is_weekend'].apply(to_string)
churn_all_w_eda['is_churn'] = churn_all_w_eda['is_churn'].apply(to_string)
churn_all_w_eda['month'] = churn_all_w_eda['created_at'].dt.month
churn_all_w_eda['year'] = churn_all_w_eda['created_at'].dt.year

"""###### Churn Percentage by Gender"""

x,y = 'cust_gender', 'is_churn'
churn_all_w_eda_gender = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_gender = churn_all_w_eda_gender.mul(100)
churn_all_w_eda_gender = churn_all_w_eda_gender.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_gender, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Gender", fontsize=16)
plt.show()

"""###### Churn Percentage by Customer Join Time"""

data.cust_join_time.quantile(.5)

data.cust_join_time.quantile(.1)

x,y = 'cust_jointime_type', 'is_churn'
churn_all_w_eda_jointime = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_jointime = churn_all_w_eda_jointime.mul(100)
churn_all_w_eda_jointime = churn_all_w_eda_jointime.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_jointime, palette="coolwarm_r",height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Join Time Type", fontsize=16)
plt.show()

"""###### Churn Percentage by Is_Jawa_Bali"""

x,y = 'is_jawabali', 'is_churn'
churn_all_w_eda_jawabali = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_jawabali = churn_all_w_eda_jawabali.mul(100)
churn_all_w_eda_jawabali = churn_all_w_eda_jawabali.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_jawabali, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Is_Jawa Bali", fontsize=16)
plt.show()

"""###### Churn Percentage by Shipment Fee"""

x,y = 'shipment_fee_type', 'is_churn'
churn_all_w_eda_feetype = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_feetype = churn_all_w_eda_feetype.mul(100)
churn_all_w_eda_feetype = churn_all_w_eda_feetype.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_feetype, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Shipment Fee Type", fontsize=16)
plt.show()

"""###### Churn Percentage by Wilayah"""

x,y = 'wilayah', 'is_churn'
churn_all_w_eda_wilayah = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_wilayah = churn_all_w_eda_wilayah.mul(100)
churn_all_w_eda_wilayah = churn_all_w_eda_wilayah.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_wilayah, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Wilayah", fontsize=16)
plt.show()

"""###### Churn Percentage by Age Group"""

x,y = 'age_group', 'is_churn'
churn_all_w_eda_agegroup = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_agegroup = churn_all_w_eda_agegroup.mul(100)
churn_all_w_eda_agegroup = churn_all_w_eda_agegroup.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_agegroup, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Age Group", fontsize=16)
plt.show()

"""###### Churn Percentage by Device Type"""

x,y = 'device_type', 'is_churn'
churn_all_w_eda_device_type = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_device_type = churn_all_w_eda_device_type.mul(100)
churn_all_w_eda_device_type = churn_all_w_eda_device_type.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_device_type, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Device Type", fontsize=16)
plt.show()

"""###### Churn Percentage by Dayname"""

x,y = 'dayname', 'is_churn'
churn_all_w_eda_dayname = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_dayname = churn_all_w_eda_dayname.mul(100)
churn_all_w_eda_dayname = churn_all_w_eda_dayname.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_dayname, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Day Name", fontsize=16)
plt.show()

"""###### Churn Percentage by Is_Weekend"""

x,y = 'is_weekend', 'is_churn'
churn_all_w_eda_isweekend = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_isweekend = churn_all_w_eda_isweekend.mul(100)
churn_all_w_eda_isweekend = churn_all_w_eda_isweekend.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_isweekend, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Is_Weekend", fontsize=16)
plt.show()

"""###### Churn Percentage by Item Price"""

x,y = 'item_price_type', 'is_churn'
churn_all_w_eda_price = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_price = churn_all_w_eda_price.mul(100)
churn_all_w_eda_price = churn_all_w_eda_price.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_price, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Item Price", fontsize=16)
plt.show()

"""###### Churn Percentage by Quantity"""

x,y = 'quantity_type', 'is_churn'
churn_all_w_eda_quantity = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_quantity = churn_all_w_eda_quantity.mul(100)
churn_all_w_eda_quantity = churn_all_w_eda_quantity.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_quantity, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Quantity", fontsize=16)
plt.show()

"""###### Churn Percentage by Daypart"""

x,y = 'daypart', 'is_churn'
churn_all_w_eda_daypart = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_daypart = churn_all_w_eda_daypart.mul(100)
churn_all_w_eda_daypart = churn_all_w_eda_daypart.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_daypart, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Day Part", fontsize=16)
plt.show()

"""###### Churn Percentage by Shipment Length Limit"""

x,y = 'shipment_length_limit', 'is_churn'
churn_all_w_eda_ship_length = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_ship_length = churn_all_w_eda_ship_length.mul(100)
churn_all_w_eda_ship_length = churn_all_w_eda_ship_length.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_ship_length, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Shipment Length", fontsize=16)
plt.show()

"""###### Churn Percentage by Payment Status"""

x,y = 'payment_status', 'is_churn'
churn_all_w_eda_payment_status = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_payment_status = churn_all_w_eda_payment_status.mul(100)
churn_all_w_eda_payment_status = churn_all_w_eda_payment_status.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_payment_status, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Payment Status", fontsize=16)
plt.show()

"""###### Churn Percentage by Payment Method"""

x,y = 'payment_method', 'is_churn'
churn_all_w_eda_payment_met = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_payment_met = churn_all_w_eda_payment_met.mul(100)
churn_all_w_eda_payment_met = churn_all_w_eda_payment_met.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_payment_met, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Payment Method", fontsize=16)
plt.show()

x,y = 'payment_method', 'payment_status'
churn_all_w_eda_payment_met_stat = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_payment_met_stat = churn_all_w_eda_payment_met_stat.mul(100)
churn_all_w_eda_payment_met_stat = churn_all_w_eda_payment_met_stat.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_payment_met_stat, palette="coolwarm_r", height=8)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Payment Method vs Payment Status", fontsize=16)
plt.show()

"""###### Churn Percentage by Month"""

x,y = 'month', 'is_churn'
churn_all_w_eda_month = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_month = churn_all_w_eda_month.mul(100)
churn_all_w_eda_month = churn_all_w_eda_month.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_month, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Month", fontsize=16)
plt.show()

"""###### Churn Percentage by Year"""

plt.rcParams["figure.figsize"] = (20,3)
x,y = 'year', 'is_churn'
churn_all_w_eda_year = churn_all_w_eda.groupby(x)[y].value_counts(normalize=True)
churn_all_w_eda_year = churn_all_w_eda_year.mul(100)
churn_all_w_eda_year = churn_all_w_eda_year.rename('percent').reset_index()

g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=churn_all_w_eda_year, palette="coolwarm_r", height=12)
g.ax.set_ylim(0,100)
for p in g.ax.patches:
    txt = str(p.get_height().round(2)) + '%'
    txt_x = p.get_x()
    txt_y = p.get_height()
    g.ax.text(txt_x,txt_y,txt)
plt.title("Churn by Year", fontsize=16)
plt.show()

plt.figure(figsize=(12,8))
sns.countplot(x='year',data=churn_all_w_eda,hue='is_churn',palette="coolwarm_r")
plt.title("Churn by Year", fontsize=16)
plt.show()

"""#### Hyperparameter Tuning"""

def evaluate_model(classifier):
    print("Train Accuracy :", accuracy_score(y_train, classifier.predict(X_train)))
    print("Train Confusion Matrix:")
    print(confusion_matrix(y_train, classifier.predict(X_train)))
    print("-"*50)
    print("Test Accuracy :", accuracy_score(y_test, classifier.predict(X_test)))
    print("Test Confusion Matrix:")
    print(confusion_matrix(y_test, classifier.predict(X_test)))

X = churn_7d_fs_encode.drop(['is_churn'], axis=1)
y = churn_7d_fs_encode[['is_churn']]
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)

model_score = pd.DataFrame([])
m_algorithm = []
m_accuracy = []
m_roc_auc = []

"""##### Logistic Regression"""

# Baseline
logreg = LogisticRegression(max_iter=10000)
logreg_base = logreg.fit(X_train, y_train.values.ravel())
y_predict = logreg_base.predict(X_test)
print('\n')
print('Baseline Logistic Regression')
print(classification_report(y_test, y_predict))

print("Accuracy :", logreg_base.score(X_test, y_test))
print("ROC_AUC :", roc_auc_score(y_test, y_predict))

lrb_accuracy = str(logreg_base.score(X_test, y_test))
lrb_rocauc = str(roc_auc_score(y_test, y_predict))
m_algorithm.append('Baseline Logistic Regression')
m_accuracy.append(lrb_accuracy)
m_roc_auc.append(lrb_rocauc)

y_train_pred = logreg_base.predict(X_train)
y_test_pred = logreg_base.predict(X_test)

evaluate_model(logreg_base)

#visualize Confusion Matrix:
cm = confusion_matrix(y_test, y_predict)
df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))
plt.figure(figsize = (28,20))
fig, ax = plt.subplots()
sns.set(font_scale=1.4)
sns.heatmap(df_cm, annot=True, fmt='g',cmap="YlGnBu" )
class_names=[0,1]
tick_marks = np.arange(len(class_names))
plt.tight_layout()
plt.title('Confusion matrix\n', y=1.1)
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
ax.xaxis.set_label_position("top")
plt.ylabel('Actual label\n')
plt.xlabel('Predicted label\n')
print("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_predict))

probs = logreg_base.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
logit_roc_auc = metrics.auc(fpr, tpr)

plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, label='Base Logistic Regression (area = %0.3f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Base Logistic Regression')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""Tuning"""

parameters = {
    'penalty': ['l1','l2'],
    'C': np.logspace(-3,3,7),
    'solver': ['newton-cg', 'lbfgs', 'liblinear']
}
logreg = LogisticRegression(max_iter=10000)
logreg_tuning = GridSearchCV(logreg,                    # model
                   param_grid = parameters,   # hyperparameters
                   scoring='roc_auc',        # metric for scoring
                   cv=10)                     # number of folds

logreg_tuning.fit(X_train,y_train.values.ravel())

print("Tuned Hyperparameters :", logreg_tuning.best_params_)
print("Accuracy :",logreg_tuning.best_score_)

# Tuned Logistic Regression
tuned_logreg = LogisticRegression(C = 0.1,
                            penalty = 'l2',
                            solver = 'lbfgs',
                            max_iter=10000)
# logreg.fit(X_train, y_train.values.ravel())
tuned_logreg.fit(X_train,y_train.values.ravel())
# y_predict = logreg.predict(X_test)
y_predict = tuned_logreg.predict(X_test)
print('\n')
print('Tuned Logistic Regression')
print(classification_report(y_test, y_predict))

print("Accuracy :", tuned_logreg.score(X_test, y_test))
print("ROC_AUC :", roc_auc_score(y_test, y_predict))

lrt_accuracy = str(tuned_logreg.score(X_test, y_test))
lrt_rocauc = str(roc_auc_score(y_test, y_predict))
m_algorithm.append('Tuned Logistic Regression')
m_accuracy.append(lrt_accuracy)
m_roc_auc.append(lrt_rocauc)

y_train_pred = tuned_logreg.predict(X_train)
y_test_pred = tuned_logreg.predict(X_test)

evaluate_model(tuned_logreg)

#visualize Confusion Matrix:

cm = confusion_matrix(y_test, y_predict)
df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))
plt.figure(figsize = (28,20))
fig, ax = plt.subplots()
sns.set(font_scale=1.4)
sns.heatmap(df_cm, annot=True, fmt='g',cmap="YlGnBu" )
class_names=[0,1]
tick_marks = np.arange(len(class_names))
plt.tight_layout()
plt.title('Confusion matrix\n', y=1.1)
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
ax.xaxis.set_label_position("top")
plt.ylabel('Actual label\n')
plt.xlabel('Predicted label\n')
print("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_predict))

probs = tuned_logreg.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
logit_roc_auc = metrics.auc(fpr, tpr)

plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, label='Tuned Logistic Regression (area = %0.3f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Tuned Logistic Regression')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""##### Random Forest"""

# Baseline
base_rf = RandomForestClassifier()
base_rf.fit(X_train,y_train.values.ravel())
y_predict = base_rf.predict(X_test)
print('\n')
print('Baseline Random Forest')
print(classification_report(y_test, y_predict))
print("Accuracy :", base_rf.score(X_test, y_test))
print("ROC_AUC :", roc_auc_score(y_test, y_predict))

rfb_accuracy = str(base_rf.score(X_test, y_test))
rfb_rocauc = str(roc_auc_score(y_test, y_predict))
m_algorithm.append('Baseline Random Forest')
m_accuracy.append(rfb_accuracy)
m_roc_auc.append(rfb_rocauc)

y_train_pred = base_rf.predict(X_train)
y_test_pred = base_rf.predict(X_test)

evaluate_model(base_rf)

#visualize Confusion Matrix:

cm = confusion_matrix(y_test, y_predict)
df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))
plt.figure(figsize = (28,20))
fig, ax = plt.subplots()
sns.set(font_scale=1.4)
sns.heatmap(df_cm, annot=True, fmt='g',cmap="YlGnBu" )
class_names=[0,1]
tick_marks = np.arange(len(class_names))
plt.tight_layout()
plt.title('Confusion matrix\n', y=1.1)
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
ax.xaxis.set_label_position("top")
plt.ylabel('Actual label\n')
plt.xlabel('Predicted label\n')
print("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_predict))

probs = base_rf.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
logit_roc_auc = metrics.auc(fpr, tpr)

plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, label='Base Random Forest (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.02, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Base Random Forest')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""Tuning"""

grid_space={'max_depth':[3,5,10,None],
              'n_estimators':[10,100,200],
              'max_features':[1,3,5,7],
              'min_samples_leaf':[1,2,3],
              'min_samples_split':[1,2,3]
           }
grid = GridSearchCV(base_rf,param_grid=grid_space,cv=3,scoring='roc_auc')
rf_grid = grid.fit(X_train,y_train.values.ravel())

# grid search results
print('Best grid search hyperparameters are: '+str(rf_grid.best_params_))
print('Best grid search score is: '+str(rf_grid.best_score_))

rf_tuned = RandomForestClassifier(max_depth=None, max_features=3, min_samples_leaf=1, min_samples_split=2, n_estimators=200)
rf_tuned.fit(X_train,y_train.values.ravel())
y_predict = rf_tuned.predict(X_test)
print('\n')
print('Tuned Random Forest')
print(classification_report(y_test, y_predict))
print("Accuracy :", rf_tuned.score(X_test, y_test))
print("ROC_AUC :", roc_auc_score(y_test, y_predict))

rft_accuracy = str(rf_tuned.score(X_test, y_test))
rft_rocauc = str(roc_auc_score(y_test, y_predict))
m_algorithm.append('Tuned Random Forest')
m_accuracy.append(rft_accuracy)
m_roc_auc.append(rft_rocauc)

y_train_pred = rf_tuned.predict(X_train)
y_test_pred = rf_tuned.predict(X_test)

evaluate_model(rf_tuned)

#visualize Confusion Matrix:

cm = confusion_matrix(y_test, y_predict)
df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))
plt.figure(figsize = (28,20))
fig, ax = plt.subplots()
sns.set(font_scale=1.4)
sns.heatmap(df_cm, annot=True, fmt='g',cmap="YlGnBu" )
class_names=[0,1]
tick_marks = np.arange(len(class_names))
plt.tight_layout()
plt.title('Confusion matrix\n', y=1.1)
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
ax.xaxis.set_label_position("top")
plt.ylabel('Actual label\n')
plt.xlabel('Predicted label\n')
print("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_predict))

probs = rf_tuned.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
logit_roc_auc = metrics.auc(fpr, tpr)

plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, label='Tuned Random Forest(area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.02, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Tuned Random Forest')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""##### Decision Tree"""

# Baseline
dt_base = DecisionTreeClassifier()
dt_base.fit(X_train,y_train.values.ravel())
y_predict = dt_base.predict(X_test)
print('\n')
print('Baseline Decision Tree')
print(classification_report(y_test, y_predict))
print("Accuracy :", dt_base.score(X_test, y_test))
print("ROC_AUC :", roc_auc_score(y_test, y_predict))

dtb_accuracy = str(dt_base.score(X_test, y_test))
dtb_rocauc = str(roc_auc_score(y_test, y_predict))
m_algorithm.append('Baseline Decision Tree')
m_accuracy.append(dtb_accuracy)
m_roc_auc.append(dtb_rocauc)

y_train_pred = dt_base.predict(X_train)
y_test_pred = dt_base.predict(X_test)

evaluate_model(dt_base)

#visualize Confusion Matrix:

cm = confusion_matrix(y_test, y_predict)
df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))
plt.figure(figsize = (28,20))
fig, ax = plt.subplots()
sns.set(font_scale=1.4)
sns.heatmap(df_cm, annot=True, fmt='g',cmap="YlGnBu" )
class_names=[0,1]
tick_marks = np.arange(len(class_names))
plt.tight_layout()
plt.title('Confusion matrix\n', y=1.1)
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
ax.xaxis.set_label_position("top")
plt.ylabel('Actual label\n')
plt.xlabel('Predicted label\n')
print("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_predict))

probs = dt_base.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)

plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, label='Base Decision Tree (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Base Decision Tree')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""Tuning"""

params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}
# Instantiate the grid search model
dt_grid = GridSearchCV(estimator=dt_base,
                           param_grid=params,
                           cv=4, n_jobs=-1, verbose=1, scoring = "roc_auc")
# %%time
dt_grid.fit(X_train, y_train)

dt_best = dt_grid.best_estimator_
dt_best

#  Tuned Decision Tree
dt_tuned = DecisionTreeClassifier(max_depth=20, min_samples_leaf=10)
dt_tuned.fit(X_train,y_train.values.ravel())
y_predict = dt_tuned.predict(X_test)
print('\n')
print('Tuned Decision Tree')
print(classification_report(y_test, y_predict))
print("Accuracy :", dt_tuned.score(X_test, y_test))
print("ROC_AUC :", roc_auc_score(y_test, y_predict))

dtt_accuracy = str(dt_tuned.score(X_test, y_test))
dtt_rocauc = str(roc_auc_score(y_test, y_predict))
m_algorithm.append('Tuned Decision Tree')
m_accuracy.append(dtt_accuracy)
m_roc_auc.append(dtt_rocauc)

y_train_pred = dt_tuned.predict(X_train)
y_test_pred = dt_tuned.predict(X_test)

evaluate_model(dt_tuned)

#visualize Confusion Matrix:

cm = confusion_matrix(y_test, y_predict)
df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))
plt.figure(figsize = (28,20))
fig, ax = plt.subplots()
sns.set(font_scale=1.4)
sns.heatmap(df_cm, annot=True, fmt='g',cmap="YlGnBu" )
class_names=[0,1]
tick_marks = np.arange(len(class_names))
plt.tight_layout()
plt.title('Confusion matrix\n', y=1.1)
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
ax.xaxis.set_label_position("top")
plt.ylabel('Actual label\n')
plt.xlabel('Predicted label\n')
print("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_predict))

probs = dt_tuned.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)

plt.figure(figsize=(10,6))
plt.plot(fpr, tpr, label='Tuned Decision Tree (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Tuned Decision Tree')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""##### Model Selection"""

md = {"Algorithm": m_algorithm, "Accuracy": m_accuracy, "ROC-AUC":m_roc_auc}
df_md = pd.DataFrame(md)
model_score = model_score.append(df_md)

model_score.sort_values(by=['Accuracy', 'ROC-AUC'], ascending=False)

"""Berdasarkan akurasi, maka model yang akan dipilih adalah **Baseline Random Forest**"""

predictions = base_rf.predict_proba(X_test)[:,1]
predictions

plt.figure(figsize=(12, 8))
plt.hist(predictions, bins = int(180/6))

plt.title('Probability Distribution of Churn Risk')
plt.xlabel('Churn Risk')
plt.ylabel('# Customers')
plt.show()

"""#### Feature Importance"""

base_rf.feature_importances_

penting = pd.DataFrame({'feature':X_train.columns, 'coef':base_rf.feature_importances_})
penting.sort_values(by='coef').sort_values(by='coef', ascending=False)

plt.figure(figsize=(12,15))
sns.barplot(data=penting.sort_values(by='coef', ascending=False), y="feature", x="coef")